# 프롬프트
> Contrastive Learning 계열은 뭐니?

대조 학습(Contrastive Learning) 계열은 '서로 비슷한 것은 가깝게, 다른 것은 멀게' 만들도록 학습하는 자기지도학습(Self-Supervised Learning) 방법론입니다. 정답이 없는 대량의 데이터로부터 스스로 데이터의 특징(representation)을 학습하게 하는 방식이죠.

쉽게 비유하자면, 여러 장의 고양이 사진과 강아지 사진이 섞여 있을 때, 컴퓨터에게 어떤 사진이 고양이인지 알려주지 않고도 "이 사진과 저 사진은 같은 종류(고양이)니 비슷하게 생각해" 그리고 "이 사진(고양이)과 저 사진(강아지)은 다른 종류니 다르게 생각해" 라고 알려주는 것과 같습니다. 이 과정을 반복하면, 모델은 자연스럽게 고양이 사진들의 공통적인 특징과 강아지 사진들의 공통적인 특징을 스스로 학습하게 됩니다.

대조 학습의 핵심 구성 요소
대조 학습은 보통 다음 세 가지 요소로 이루어집니다.

1. 데이터 증강 (Data Augmentation)
하나의 원본 데이터(Anchor)에 약간의 변형을 주어 내용은 같지만 형태는 다른 여러 버전의 데이터를 만듭니다. 예를 들어, 시계열 데이터라면 특정 구간을 잘라내거나(cropping), 노이즈를 추가하거나, 속도를 빠르게 하는 등의 변형을 가합니다. 이렇게 원본에서 파생된 데이터 쌍을 **긍정 쌍 (Positive Pair)**이라고 부릅니다. 이 둘은 '비슷하게' 여겨야 할 대상입니다.

2. 긍정 쌍 (Positive Pair) vs 부정 쌍 (Negative Pair)
긍정 쌍 (Positive Pair): 같은 데이터로부터 파생된 쌍 (Anchor + Augmented Data). 모델이 '가깝게' 만들어야 할 대상입니다.

부정 쌍 (Negative Pair): 앵커와 전혀 상관없는 다른 데이터들. 모델이 '멀게' 만들어야 할 대상입니다.

3. 인코더(Encoder)와 손실 함수(Loss Function)
인코더 (Encoder): 입력된 데이터를 저차원의 벡터(임베딩)로 압축하는 역할을 하는 신경망 모델입니다. 대조 학습의 목표는 이 인코더가 좋은 임베딩을 만들도록 훈련시키는 것입니다.

대조 손실 (Contrastive Loss): 인코더가 출력한 벡터들을 보고, 긍정 쌍의 벡터는 거리가 가까워지도록, 부정 쌍의 벡터는 거리가 멀어지도록 점수를 계산합니다. 모델은 이 점수(손실 값)를 낮추는 방향으로 인코더를 업데이트하며 학습을 진행합니다.

시계열 데이터에서의 대조 학습 (예: TS2Vec)
사용자님의 과제에 언급된 TS2Vec 같은 모델은 이러한 대조 학습을 시계열 데이터에 적용한 대표적인 예입니다.

하나의 시계열 데이터에서 서로 겹치는 두 개의 시간 구간을 임의로 잘라냅니다. 이 두 구간은 같은 데이터에서 나왔으므로 긍정 쌍이 됩니다.

이 두 구간을 인코더에 넣어 각각 임베딩 벡터를 얻습니다.

동시에, 데이터셋의 다른 시계열 데이터에서 잘라낸 구간들(부정 쌍)의 임베딩 벡터도 계산합니다.

대조 손실을 이용해, 같은 시계열에서 나온 긍정 쌍의 임베딩은 서로 가까워지게, 다른 시계열에서 온 부정 쌍의 임베딩과는 멀어지도록 인코더를 학습시킵니다.

이 과정을 통해 모델은 각 시계열 데이터의 고유한 패턴과 특징을 담고 있는 '표현(representation)'을 스스로 학습하게 되는 것입니다. 이렇게 학습된 인코더는 나중에 분류, 예측, 이상탐지 등 다양한 다운스트림 태스크에 매우 효과적으로 사용될 수 있습니다.