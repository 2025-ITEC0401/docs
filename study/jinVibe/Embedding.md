# 프롬프트
> 임베딩의 개념과 종류를 알려줘

임베딩은 컴퓨터가 이해하기 어려운 **고차원 데이터를 저차원의 연속적인 벡터로 변환**하는 과정을 말합니다. 🤖 이렇게 변환된 벡터는 데이터의 특징과 의미적 관계를 보존하고 있어, 컴퓨터가 효율적으로 처리하고 분석할 수 있게 됩니다.

### 임베딩의 주요 특징
* **저차원**: 수백만 개의 단어로 이루어진 문장을 수백 차원의 벡터로 압축합니다. 이는 메모리 사용을 줄이고 계산 속도를 높이는 데 도움이 됩니다.
* **연속성**: 벡터 공간에서 의미적으로 유사한 데이터는 서로 가까운 위치에 존재합니다. 예를 들어, '남자'와 '왕', '여자'와 '여왕'은 서로 유사한 관계를 가집니다.
* **밀집성(Dense)**: 대부분의 벡터 요소가 0이 아닌 값을 가집니다. 이는 희소한(Sparse) 표현 방식인 원-핫 인코딩(One-hot encoding)과 대비됩니다.

---

### 임베딩의 종류
임베딩은 크게 **Word Embedding**, **Sentence Embedding**, **Graph Embedding**, **Knowledge Graph Embedding**, **Multimodal Embedding** 등으로 나눌 수 있습니다.

#### 1. Word Embedding (단어 임베딩)
단어를 벡터로 변환하는 가장 기본적인 임베딩 방식입니다. 단어의 의미적, 문법적 관계를 포착하는 데 중점을 둡니다.
* **Word2Vec**: 단어 주변의 단어를 예측하는 방식(CBOW) 또는 현재 단어를 이용해 주변 단어를 예측하는 방식(Skip-gram)을 사용합니다.
* **GloVe**: 단어의 동시 출현 빈도(Co-occurrence)를 기반으로 학습하며, 전역적인 통계 정보를 반영합니다.
* **FastText**: 단어를 n-gram 단위로 분해하여 임베딩을 생성합니다. 이는 오타나 희귀 단어에 강점을 가집니다.

#### 2. Contextual Embedding (문맥적 임베딩)
단어 하나에 여러 의미가 있을 수 있는 점을 해결하기 위해 문맥을 고려하여 임베딩을 생성합니다.
* **ELMo**: 문장의 순방향과 역방향 언어 모델을 결합하여 각 단어의 문맥에 맞는 임베딩을 생성합니다.
* **BERT (Bidirectional Encoder Representations from Transformers)**: 문장의 양방향 문맥을 모두 고려하여 단어 임베딩을 생성합니다. 'Masked Language Model'과 'Next Sentence Prediction'을 통해 사전 학습됩니다.
* **GPT (Generative Pre-trained Transformer)**: 단방향으로 문장을 생성하는 데 특화된 모델로, 다음에 올 단어를 예측하는 방식으로 학습합니다. 

#### 3. Sentence Embedding (문장 임베딩)
단어의 순서와 관계를 포함하여 문장 전체의 의미를 하나의 벡터로 표현합니다.
* **InferSent**: 자연어 추론(Natural Language Inference) 태스크를 통해 학습된 문장 임베딩 모델입니다.
* **Universal Sentence Encoder**: 다양한 태스크에 대해 높은 성능을 보이는 다용도 문장 임베딩 모델입니다.

#### 4. Graph Embedding (그래프 임베딩)
그래프의 노드와 엣지 관계를 벡터로 변환하는 기술입니다.
* **Node2Vec**: 노드의 이웃 노드를 예측하는 방식으로 학습하며, 랜덤 워크를 통해 노드 시퀀스를 생성합니다.
* **GraphSAGE**: 샘플링된 이웃 노드의 특징을 집계하여 노드 임베딩을 생성합니다.

#### 5. Multimodal Embedding (다중 모드 임베딩)
텍스트, 이미지, 오디오 등 여러 종류의 데이터를 하나의 벡터 공간에 매핑합니다. 이를 통해 다른 종류의 데이터 간의 관계를 분석할 수 있습니다. 예를 들어, 이미지와 이미지를 설명하는 텍스트를 같은 벡터 공간에 임베딩하여 유사도를 측정할 수 있습니다.

| 임베딩 종류 | 입력 데이터 | 출력 특징 | 활용 예시 |
| :--- | :--- | :--- | :--- |
| **Word Embedding** | 단어 | 단어의 의미/문법적 관계 | 챗봇, 단어 유사도 검색 |
| **Contextual Embedding** | 문장 속 단어 | 문맥에 따른 단어의 의미 | 자연어 이해(NLU), 질의 응답 |
| **Sentence Embedding** | 문장 | 문장 전체의 의미 | 텍스트 분류, 문장 유사도 |
| **Graph Embedding** | 그래프 노드/엣지 | 노드 간의 관계 | 소셜 네트워크 분석, 추천 시스템 |
| **Multimodal Embedding** | 텍스트/이미지 등 | 데이터 간의 공통된 의미 | 이미지-텍스트 검색 |



