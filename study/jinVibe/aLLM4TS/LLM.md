Q1. 텍스트 데이터로 학습된 LLM(GPT, Llama 등)이 어떻게 전혀 다른 성격의 시계열 데이터를 이해하고 예측할 수 있는가?

A: '보편적 패턴 인식 능력' 때문입니다. 언어 모델이 학습한 문법적 구조나 순차적 패턴(상승, 하강, 주기성 등)은 시계열 데이터의 추세나 계절성과 수학적으로 유사성을 가집니다. aLLM4TS와 같은 연구는 **시계열 데이터를 LLM이 이해할 수 있는 토큰 공간으로 투영(Projection)**하거나, LLM의 일부 파라미터만 미세 조정(Fine-tuning)하여 언어 모델의 지식을 시계열 도메인으로 **재프로그래밍(Reprogramming)**하는 원리를 사용합니다.

Q2. aLLM4TS의 2단계 학습(Two-stage Fine-tuning) 전략은 범용 프레임워크 설계에 어떤 시사점을 주는가?

A: aLLM4TS는 1단계에서 시계열 패치를 LLM의 입력 공간에 맞추는 경량 학습을 수행하고, 2단계에서 특정 태스크에 맞게 튜닝합니다. 범용 프레임워크에서도 "도메인 불가지론적(Domain-agnostic)인 인코더 학습" 단계와 "태스크 특화(Task-specific) 헤드 학습" 단계를 분리하는 것이 효율적임을 시사합니다. 즉, 모든 도메인의 데이터를 아우르는 '기초 모델(Foundation Model)'을 먼저 만들고, 사용처에 따라 가볍.게 튜닝하는 구조를 설계해야 합니다