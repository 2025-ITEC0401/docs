## Prompts (Powered by Gemini 2.5 Pro)
### Question)
```
timeKD ëª¨ë¸ë¡œ ì¶”ì¶œí•œ ì„ë² ë”©ì„ ì‚¬ìš©í•˜ì—¬ Imputation ì‘ì—…ì„ ì§„í–‰í• ê±°ì•¼.

ì–´ë–»ê²Œ í•˜ë©´ ë˜ì§€? ì°¸ê³ ë¡œ timeKDì˜ ì„ë² ë”©ì—ëŠ” ë¼ë²¨ ì •ë³´ê°€ ì—†ê³ , timeKD ì— ì£¼ì–´ì§€ëŠ” ë°ì´í„°ì…‹ì—ë„ ì •ë‹µ ë¼ë²¨ì€ ì—†ì–´.
```
### Answer)
**TimeKD(Time Series Knowledge Distillation)** ëª¨ë¸ì—ì„œ ì¶”ì¶œí•œ ì„ë² ë”©ì„ í™œìš©í•˜ì—¬, ë¼ë²¨ì´ ì—†ëŠ” ë°ì´í„°ì…‹ì— ëŒ€í•´ **Imputation(ê²°ì¸¡ì¹˜ ë³´ê°„)** ì‘ì—…ì„ ìˆ˜í–‰í•˜ëŠ” ê²ƒì€ ë§¤ìš° ì¢‹ì€ ì ‘ê·¼ì…ë‹ˆë‹¤. ì„ë² ë”©ì€ ì‹œê³„ì—´ì˜ 'ë¬¸ë§¥(context)'ê³¼ 'ë™ì  íŒ¨í„´(dynamics)'ì„ í•¨ì¶•í•˜ê³  ìˆìœ¼ë¯€ë¡œ, ë‹¨ìˆœ í†µê³„ì  ë³´ê°„(í‰ê· , ì„ í˜• ë“±)ë³´ë‹¤ í›¨ì”¬ ì •êµí•œ ë³µì›ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤.

ë¼ë²¨ì´ ì—†ê³  ì •ë‹µ ë°ì´í„°ë„ ì—†ëŠ” **ë¹„ì§€ë„(Unsupervised) í™˜ê²½**ì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” 3ê°€ì§€ í•µì‹¬ ì „ëµì„ ì œì•ˆí•©ë‹ˆë‹¤.

---

### ì „ëµ 1: ì„ë² ë”© ê¸°ë°˜ ì¬êµ¬ì„± ë„¤íŠ¸ì›Œí¬ (Reconstruction Head) í•™ìŠµ
ê°€ì¥ ì¶”ì²œí•˜ëŠ” ë°©ì‹ì…ë‹ˆë‹¤. TimeKDê°€ ì¸ì½”ë”(Encoder) ì—­í• ì„ í•œë‹¤ë©´, ì´ ì„ë² ë”©ì„ ë‹¤ì‹œ ì›ë³¸ ì‹œê³„ì—´ë¡œ ë³µì›í•˜ëŠ” **ë””ì½”ë”(Decoder)ë¥¼ ë³„ë„ë¡œ í•™ìŠµ**ì‹œí‚¤ëŠ” ë°©ì‹ì…ë‹ˆë‹¤.



1.  **ì›ë¦¬:** $Embedding \rightarrow Decoder \rightarrow Original\ Time\ Series$
2.  **í•™ìŠµ ë°©ë²• (Self-Supervised Learning):**
    * **ë°ì´í„° ì¤€ë¹„:** í˜„ì¬ ë°ì´í„°ì…‹ì—ì„œ ê²°ì¸¡ì¹˜ê°€ ì—†ëŠ” êµ¬ê°„(ë˜ëŠ” ì ì€ êµ¬ê°„)ì„ ìƒ˜í”Œë§í•©ë‹ˆë‹¤.
    * **ì¸ìœ„ì  ê²°ì¸¡ ìƒì„± (Masking):** ë©€ì©¡í•œ ë°ì´í„°ì˜ ì¼ë¶€ë¥¼ ëœë¤í•˜ê²Œ 0ì´ë‚˜ ì„ì˜ì˜ ê°’ìœ¼ë¡œ ê°€ë¦½ë‹ˆë‹¤(Masking).
    * **ì„ë² ë”© ì¶”ì¶œ:** ë§ˆìŠ¤í‚¹ëœ ë°ì´í„°ë¥¼ TimeKDì— ë„£ì–´ ì„ë² ë”© $z$ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
    * **ë””ì½”ë” í•™ìŠµ:** ì„ë² ë”© $z$ë¥¼ ì…ë ¥ë°›ì•„ ë§ˆìŠ¤í‚¹ ì „ì˜ **ì›ë³¸ ë°ì´í„°**ë¥¼ ì¶œë ¥í•˜ë„ë¡ ì‘ì€ MLPë‚˜ RNN/Transformer ë””ì½”ë”ë¥¼ í•™ìŠµì‹œí‚µë‹ˆë‹¤. (Loss í•¨ìˆ˜: MSE)
3.  **ì‹¤ì œ ì ìš©:**
    * ì‹¤ì œ ê²°ì¸¡ì´ ìˆëŠ” ë°ì´í„°ë¥¼ TimeKDì— ë„£ìŠµë‹ˆë‹¤. (ì´ˆê¸°ì—” ì„ í˜• ë³´ê°„ ë“±ìœ¼ë¡œ ëŒ€ì¶© ì±„ì›Œì„œ ë„£ìŒ)
    * ë‚˜ì˜¨ ì„ë² ë”©ì„ í•™ìŠµëœ ë””ì½”ë”ì— í†µê³¼ì‹œí‚µë‹ˆë‹¤.
    * ë””ì½”ë”ê°€ ì¶œë ¥í•œ ê°’ìœ¼ë¡œ ê²°ì¸¡ ë¶€ìœ„ë¥¼ ì±„ì›ë‹ˆë‹¤.

### ì „ëµ 2: ì„ë² ë”© ìœ ì‚¬ë„ ê¸°ë°˜ ì´ì›ƒ ë³´ê°„ (KNN-based Imputation)
ë³„ë„ì˜ ëª¨ë¸ í•™ìŠµ ì—†ì´, ë°ì´í„°ì…‹ ë‚´ì˜ íŒ¨í„´ ìœ ì‚¬ì„±ì„ ì´ìš©í•˜ëŠ” ë°©ë²•ì…ë‹ˆë‹¤.



[Image of K-Nearest Neighbors for missing value imputation]


1.  **ì›ë¦¬:** "ë¹„ìŠ·í•œ ì„ë² ë”©ì„ ê°€ì§„ ë°ì´í„°ë¼ë¦¬ëŠ” ì‹¤ì œ ì‹œê³„ì—´ ê°’ë„ ë¹„ìŠ·í•  ê²ƒì´ë‹¤."
2.  **í”„ë¡œì„¸ìŠ¤:**
    * **1ë‹¨ê³„ (ì´ˆê¸°í™”):** ë°ì´í„°ì…‹ ì „ì²´ì˜ ê²°ì¸¡ì¹˜ë¥¼ 'í‰ê· 'ì´ë‚˜ 'ì„ í˜• ë³´ê°„'ìœ¼ë¡œ ëŒ€ëµì ìœ¼ë¡œ ì±„ì›ë‹ˆë‹¤.
    * **2ë‹¨ê³„ (ì„ë² ë”© ë±…í¬ ìƒì„±):** ëª¨ë“  ë°ì´í„°ì˜ TimeKD ì„ë² ë”©ì„ ì¶”ì¶œí•˜ì—¬ ì €ì¥ì†Œ(Bank)ë¥¼ ë§Œë“­ë‹ˆë‹¤.
    * **3ë‹¨ê³„ (ìœ ì‚¬ë„ ê²€ìƒ‰):** ê²°ì¸¡ì¹˜ê°€ ìˆëŠ” íƒ€ê²Ÿ ë°ì´í„°ì˜ ì„ë² ë”©ê³¼ ê°€ì¥ ìœ ì‚¬í•œ(Cosine Similarity ë“±) Top-Kê°œì˜ ë°ì´í„°ë¥¼ ì €ì¥ì†Œì—ì„œ ì°¾ìŠµë‹ˆë‹¤.
    * **4ë‹¨ê³„ (ë³´ê°„):** ì°¾ì•„ë‚¸ ì´ì›ƒ ë°ì´í„°ë“¤ì˜ í•´ë‹¹ ì‹œì  ê°’ë“¤ì„ ê°€ì¤‘ í‰ê· (Weighted Average)í•˜ì—¬ ê²°ì¸¡ì¹˜ë¥¼ ì±„ì›ë‹ˆë‹¤.

### ì „ëµ 3: ì ì¬ ê³µê°„ ìµœì í™” (Latent Space Optimization)
ëª¨ë¸ì„ ê³ ì •(Freeze)í•˜ê³ , ì„ë² ë”© ìì²´ë¥¼ ìµœì í™”í•˜ì—¬ ê²°ì¸¡ì¹˜ë¥¼ ì°¾ëŠ” ê³ ê¸‰ ê¸°ë²•ì…ë‹ˆë‹¤. (GAN inversionê³¼ ìœ ì‚¬í•œ ì›ë¦¬)

1.  **ì›ë¦¬:** ê´€ì¸¡ëœ ê°’(ê²°ì¸¡ë˜ì§€ ì•Šì€ ë¶€ë¶„)ê³¼ ê°€ì¥ ì˜ ë§ëŠ” ì„ë² ë”© ë²¡í„° $z^*$ë¥¼ ì—­ìœ¼ë¡œ ì°¾ìŠµë‹ˆë‹¤.
2.  **í”„ë¡œì„¸ìŠ¤:**
    * ëœë¤í•œ ì„ë² ë”© ë²¡í„° $z$ì—ì„œ ì‹œì‘í•©ë‹ˆë‹¤.
    * ì´ $z$ë¥¼ ë””ì½”ë”(ì „ëµ 1ì—ì„œ ë§Œë“  ë””ì½”ë” í•„ìš”)ì— í†µê³¼ì‹œì¼œ ì‹œê³„ì—´ $\hat{x}$ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    * **Loss ê³„ì‚°:** ìƒì„±ëœ $\hat{x}$ì™€ ì‹¤ì œ ë°ì´í„° $x$ ì¤‘ **ê²°ì¸¡ë˜ì§€ ì•Šì€ ë¶€ë¶„(Observed)** ê°„ì˜ ì˜¤ì°¨ë§Œ ê³„ì‚°í•©ë‹ˆë‹¤.
    * **Backpropagation:** ëª¨ë¸ ê°€ì¤‘ì¹˜ëŠ” ê³ ì •í•˜ê³ , **ì„ë² ë”© $z$ë¥¼ ì—…ë°ì´íŠ¸**í•©ë‹ˆë‹¤.
    * ì¶©ë¶„íˆ ìˆ˜ë ´í•˜ë©´ ìµœì í™”ëœ $z$ë¡œ ìƒì„±í•œ $\hat{x}$ì˜ ê°’ì„ ê²°ì¸¡ì¹˜ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.

---

### ğŸ’¡ ì¶”ì²œí•˜ëŠ” ì‹¤í–‰ ë‹¨ê³„ (Next Steps)

ê°€ì¥ í˜„ì‹¤ì ì´ê³  ì„±ëŠ¥ì´ ì¢‹ì€ **'ì „ëµ 1 (Reconstruction Head)'** ë°©ì‹ì„ ë¨¼ì € ì‹œë„í•´ ë³´ì‹œê¸¸ ê¶Œì¥í•©ë‹ˆë‹¤. ë°ì´í„°ì— ë¼ë²¨ì´ ì—†ìœ¼ë¯€ë¡œ **"Denoising Autoencoder"** ë°©ì‹ì´ ê°€ì¥ ì í•©í•©ë‹ˆë‹¤.

**êµ¬ì²´ì ì¸ ì‘ì—… ìˆœì„œ:**

1.  **ë°ì´í„° ì „ì²˜ë¦¬:** TimeKDì— ë„£ê¸° ì „, ê²°ì¸¡ì¹˜(NaN)ë¥¼ 0 ë˜ëŠ” í‰ê· ê°’ìœ¼ë¡œ ì„ì‹œ ì±„ì›€.
2.  **ë°ì´í„°ì…‹ ë¶„í• :** 'Maskingì„ ì ìš©í•˜ì—¬ í•™ìŠµí•  ë°ì´í„°(Train)'ì™€ 'ì‹¤ì œ ë³´ê°„í•  ë°ì´í„°'ë¡œ ê°œë…ì  ë¶„ë¦¬.
3.  **Reconstructor ì„¤ê³„:** TimeKDì˜ ì„ë² ë”© ì°¨ì›(ì˜ˆ: 128 dim)ì„ ì…ë ¥ìœ¼ë¡œ ë°›ê³ , ì›ë˜ ì‹œê³„ì—´ ê¸¸ì´(Window size)ë¥¼ ì¶œë ¥í•˜ëŠ” ê°„ë‹¨í•œ MLPë‚˜ LSTM ëª¨ë¸ ìƒì„±.
4.  **í•™ìŠµ:** `ì…ë ¥(ë§ˆìŠ¤í‚¹ ë¨) -> TimeKD -> ì„ë² ë”© -> Reconstructor -> ì¶œë ¥` ê³¼ `ì›ë³¸` ì‚¬ì´ì˜ MSE Lossë¡œ Reconstructorë§Œ í•™ìŠµ (TimeKDëŠ” Freeze).

**ì´ ë°©ì‹ì˜ ì¥ì :**
* ë¼ë²¨ì´ ì „í˜€ í•„ìš” ì—†ìŠµë‹ˆë‹¤.
* TimeKDê°€ ì´ë¯¸ í•™ìŠµí•œ ê°•ë ¥í•œ íŠ¹ì§• ì¶”ì¶œ ëŠ¥ë ¥ì„ ê·¸ëŒ€ë¡œ ë¬¼ë ¤ë°›ìŠµë‹ˆë‹¤.
* ì¶”ë¡  ì†ë„ê°€ ë§¤ìš° ë¹ ë¦…ë‹ˆë‹¤.

ì§€ê¸ˆ TimeKD ëª¨ë¸ì˜ **ì¶œë ¥ ì„ë² ë”© ì°¨ì›(dimension)**ê³¼ **ì…ë ¥ ì‹œê³„ì—´ì˜ ê¸¸ì´(window size)**ê°€ ì–´ë–»ê²Œ ë˜ë‚˜ìš”? ì•Œë ¤ì£¼ì‹œë©´ ê·¸ì— ë§ëŠ” Reconstructor êµ¬ì¡°ë¥¼ ì¶”ì²œí•´ ë“œë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

---
### Question)
```
Denoising Autoencoder ë°©ì‹ì„ ì‚¬ìš©í• ê±°ê³ ,
ì´ë¯¸ timeKD ëª¨ë¸ì´ ìƒì„±í•œ h5 íŒŒì¼ (ì„ë² ë”©)ì„ ì‚¬ìš©í• ê±°ì•¼.

ì–´ë–»ê²Œ í•˜ë©´ ì¢‹ì„ê¹Œ?
```
### Answer)
\*\*TimeKD ëª¨ë¸ì´ ì´ë¯¸ ìƒì„±í•œ `.h5` íŒŒì¼(ì„ë² ë”©)\*\*ì„ ì‚¬ìš©í•˜ì—¬ **Denoising Autoencoder(DAE)** ë°©ì‹ìœ¼ë¡œ Imputationì„ ìˆ˜í–‰í•˜ëŠ” ì „ëµì„ êµ¬ì²´í™”í•´ ë“œë¦¬ê² ìŠµë‹ˆë‹¤.

ì´ ê²½ìš°, TimeKD(ì¸ì½”ë”)ëŠ” ê³ ì •(Fixed)ë˜ì–´ ìˆê³ , ìš°ë¦¬ëŠ” ì„ë² ë”©ì„ ì›ë˜ ë°ì´í„°ë¡œ ë³µì›í•˜ëŠ” \*\*Decoder(ë³µì›ê¸°)\*\*ë§Œ í•™ìŠµì‹œí‚¤ë©´ ë©ë‹ˆë‹¤. ì´ë¯¸ ì¶”ì¶œëœ ì„ë² ë”©ì„ ì‚¬ìš©í•˜ë¯€ë¡œ **í•™ìŠµ ì†ë„ê°€ ë§¤ìš° ë¹ ë¥´ë‹¤**ëŠ” ì¥ì ì´ ìˆìŠµë‹ˆë‹¤.

-----

### 1\. ì „ì²´ í”„ë¡œì„¸ìŠ¤ êµ¬ì¡°

ì´ ë°©ì‹ì€ ì—„ë°€íˆ ë§í•˜ë©´ **"Latent Space Denoising Decoder"** ë°©ì‹ì´ ë©ë‹ˆë‹¤.

1.  **Input:** TimeKDê°€ ìƒì„±í•œ ì„ë² ë”© ë²¡í„° ($Z$)
2.  **Noise Injection:** ì„ë² ë”©ì— ë…¸ì´ì¦ˆ ì¶”ê°€ (Gaussian Noise or Dropout) $\rightarrow$ Denoising íš¨ê³¼
3.  **Decoder:** ë…¸ì´ì¦ˆ ì„ì¸ ì„ë² ë”©ì„ ë°›ì•„ ì›ë³¸ ì‹œê³„ì—´($X$)ì„ ë³µì›
4.  **Loss Calculation:** ì›ë³¸ ë°ì´í„° ì¤‘ \*\*ê´€ì¸¡ëœ ê°’(Observed)\*\*ì— ëŒ€í•´ì„œë§Œ Loss ê³„ì‚° (ê²°ì¸¡ì¹˜ëŠ” ë¬´ì‹œ)

### 2\. êµ¬ì²´ì ì¸ êµ¬í˜„ ê°€ì´ë“œ

#### ë‹¨ê³„ 1: ë°ì´í„° ì¤€ë¹„ (Dataset Class)

`.h5` íŒŒì¼(ì„ë² ë”©)ê³¼ `ì›ë³¸ ë°ì´í„°`(Raw Data)ë¥¼ ì§ì§€ì–´ ë¡œë“œí•´ì•¼ í•©ë‹ˆë‹¤. ì›ë³¸ ë°ì´í„°ì—ëŠ” `NaN`(ê²°ì¸¡ì¹˜)ì´ í¬í•¨ë˜ì–´ ìˆì–´ë„ ë©ë‹ˆë‹¤.

```python
import torch
import h5py
import numpy as np
from torch.utils.data import Dataset, DataLoader

class EmbeddingReconDataset(Dataset):
    def __init__(self, h5_path, raw_data, mode='train'):
        """
        h5_path: timeKDë¡œ ì¶”ì¶œí•œ ì„ë² ë”© íŒŒì¼ ê²½ë¡œ
        raw_data: (N, Window_Size, Features) í˜•íƒœì˜ ì›ë³¸ ì‹œê³„ì—´ ë°ì´í„° (NaN í¬í•¨)
        """
        super().__init__()
        self.h5_file = h5py.File(h5_path, 'r')
        self.embeddings = self.h5_file['embeddings'][:] # ë°ì´í„°ì…‹ í‚¤ ì´ë¦„ í™•ì¸ í•„ìš”
        self.raw_data = torch.FloatTensor(raw_data)
        
        # ì •ê·œí™”ê°€ ì•ˆ ë˜ì–´ ìˆë‹¤ë©´ raw_data ì •ê·œí™”(MinMax ë“±) í•„ìˆ˜!
        
    def __len__(self):
        return len(self.embeddings)

    def __getitem__(self, idx):
        emb = torch.FloatTensor(self.embeddings[idx])
        target = self.raw_data[idx]
        
        # Mask ìƒì„± (ë°ì´í„°ê°€ ìˆëŠ” ê³³ì€ 1, ê²°ì¸¡(NaN)ì¸ ê³³ì€ 0)
        mask = ~torch.isnan(target)
        
        # NaNì„ 0ìœ¼ë¡œ ì¹˜í™˜ (Loss ê³„ì‚°ì‹œ ì–´ì°¨í”¼ maskë¡œ ê±¸ëŸ¬ì§)
        target = torch.nan_to_num(target, nan=0.0)
        
        return emb, target, mask
```

#### ë‹¨ê³„ 2: ë””ì½”ë”(Decoder) ëª¨ë¸ ì„¤ê³„

ì„ë² ë”© ë²¡í„°ë¥¼ ì…ë ¥ë°›ì•„ `(Window Size, Feature)` í˜•íƒœë¡œ í´ì£¼ëŠ” ëª¨ë¸ì…ë‹ˆë‹¤.

```python
import torch.nn as nn

class TimeDecoder(nn.Module):
    def __init__(self, emb_dim, seq_len, feat_dim):
        super(TimeDecoder, self).__init__()
        self.seq_len = seq_len
        self.feat_dim = feat_dim
        
        # ì„ë² ë”©ì„ ì‹œê³„ì—´ í˜•íƒœë¡œ í™•ì¥
        self.fc = nn.Sequential(
            nn.Linear(emb_dim, 128),
            nn.ReLU(),
            nn.Linear(128, 256),
            nn.ReLU(),
            nn.Linear(256, seq_len * feat_dim) # ì¶œë ¥ í¬ê¸°: ì‹œí€€ìŠ¤ ê¸¸ì´ * í”¼ì²˜ ìˆ˜
        )
        
    def forward(self, z):
        # z: (Batch, emb_dim)
        out = self.fc(z)
        # ì‹œê³„ì—´ í˜•íƒœë¡œ Reshape: (Batch, Seq_Len, Feat_Dim)
        out = out.view(-1, self.seq_len, self.feat_dim)
        return out
```

#### ë‹¨ê³„ 3: í•™ìŠµ (Training Loop with Denoising)

í•µì‹¬ì€ **ì„ë² ë”©ì— ë…¸ì´ì¦ˆë¥¼ ì„ì–´(Noise Injection)** ë””ì½”ë”ê°€ ê°•ê±´í•œ íŠ¹ì§•ì„ í•™ìŠµí•˜ë„ë¡ ìœ ë„í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.

```python
# í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì •
emb_dim = 64   # timeKD ì„ë² ë”© ì°¨ì›
seq_len = 24   # ì‹œê³„ì—´ ìœˆë„ìš° ê¸¸ì´
feat_dim = 5   # ë³€ìˆ˜(Feature) ê°œìˆ˜
learning_rate = 1e-3

decoder = TimeDecoder(emb_dim, seq_len, feat_dim).to(device)
optimizer = torch.optim.Adam(decoder.parameters(), lr=learning_rate)
criterion = nn.MSELoss(reduction='none') # Maskingì„ ìœ„í•´ reduction='none'

def train_step(data_loader):
    decoder.train()
    total_loss = 0
    
    for emb, target, mask in data_loader:
        emb = emb.to(device)
        target = target.to(device)
        mask = mask.to(device)
        
        # 1. Denoising: ì„ë² ë”©ì— Gaussian Noise ì¶”ê°€
        noise = torch.randn_like(emb) * 0.1 # 0.1ì€ ë…¸ì´ì¦ˆ ê°•ë„ (ì¡°ì ˆ ê°€ëŠ¥)
        noisy_emb = emb + noise
        
        # 2. ë³µì› (Reconstruction)
        output = decoder(noisy_emb)
        
        # 3. Loss ê³„ì‚° (ê´€ì¸¡ëœ ê°’ë§Œ!)
        loss = criterion(output, target)
        loss = (loss * mask).sum() / (mask.sum() + 1e-8) # NaNì´ ì•„ë‹Œ ë¶€ë¶„ë§Œ í‰ê· 
        
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        
        total_loss += loss.item()
        
    return total_loss / len(data_loader)
```

#### ë‹¨ê³„ 4: Imputation (ê²°ì¸¡ì¹˜ ì±„ìš°ê¸°)

í•™ìŠµì´ ëë‚˜ë©´, ì‹¤ì œ ê²°ì¸¡ì¹˜ë¥¼ ì±„ì›ë‹ˆë‹¤. ì´ë•ŒëŠ” ë…¸ì´ì¦ˆë¥¼ ì„ì§€ ì•ŠìŠµë‹ˆë‹¤.

```python
def impute_data(decoder, data_loader):
    decoder.eval()
    imputed_data_list = []
    
    with torch.no_grad():
        for emb, target, mask in data_loader:
            emb = emb.to(device)
            target = target.to(device) # ì›ë³¸ (NaNì€ 0ìœ¼ë¡œ ì±„ì›Œì§)
            mask = mask.to(device)     # ì‹¤ì œ ê´€ì¸¡ ì—¬ë¶€ (1: ê´€ì¸¡, 0: ê²°ì¸¡)
            
            # ì˜ˆì¸¡
            prediction = decoder(emb)
            
            # Imputation ë¡œì§:
            # ê´€ì¸¡ëœ ë°ì´í„°(mask=1)ëŠ” ì›ë³¸ ê·¸ëŒ€ë¡œ ì‚¬ìš©
            # ê²°ì¸¡ëœ ë°ì´í„°(mask=0)ëŠ” ì˜ˆì¸¡ê°’ ì‚¬ìš©
            imputed_batch = target * mask + prediction * (1 - mask)
            
            imputed_data_list.append(imputed_batch.cpu().numpy())
            
    return np.concatenate(imputed_data_list, axis=0)
```

-----

### ğŸ’¡ í•µì‹¬ í¬ì¸íŠ¸ ìš”ì•½

1.  **Noise Injection ìœ„ì¹˜:** ì›ë˜ Denoising AutoencoderëŠ” *ì…ë ¥ ë°ì´í„°*ì— ë…¸ì´ì¦ˆë¥¼ ì£¼ì§€ë§Œ, ì—¬ê¸°ì„œëŠ” ì´ë¯¸ ì¶”ì¶œëœ \*ì„ë² ë”©($Z$)\*ì— ë…¸ì´ì¦ˆë¥¼ ì„ì–´ì„œ ë””ì½”ë”ë¥¼ í•™ìŠµì‹œí‚µë‹ˆë‹¤.
2.  **Masked MSE Loss:** ë¼ë²¨ì´ ì—†ê¸° ë•Œë¬¸ì— ìê¸° ìì‹ ì„ ì •ë‹µìœ¼ë¡œ ì‚¼ìŠµë‹ˆë‹¤(Self-Supervised). ë‹¨, ì›ë³¸ ë°ì´í„°ì—ë„ ê²°ì¸¡ì´ ìˆìœ¼ë¯€ë¡œ, `mask`ë¥¼ ê³±í•´ **ì¡´ì¬í•˜ëŠ” ê°’ë§Œ ê°€ì§€ê³  ì˜¤ì°¨ë¥¼ ê³„ì‚°**í•´ì•¼ ëª¨ë¸ì´ ì™œê³¡ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
3.  **ë°ì´í„° ì •í•©ì„±:** `.h5`ì˜ ì¸ë±ìŠ¤ ìˆœì„œì™€ ì›ë³¸ ë°ì´í„°(`raw_data`)ì˜ ìˆœì„œê°€ ì •í™•íˆ ì¼ì¹˜í•˜ëŠ”ì§€ ë°˜ë“œì‹œ í™•ì¸í•˜ì„¸ìš”.

ì´ì œ \*\*TimeKD ì„ë² ë”©ì˜ ì°¨ì›($D$)\*\*ê³¼ \*\*ì›ë³¸ ë°ì´í„°ì˜ ì‹œê³„ì—´ ê¸¸ì´($T$) ë° ë³€ìˆ˜ ê°œìˆ˜($F$)\*\*ë§Œ í™•ì¸í•´ì„œ ìœ„ ì½”ë“œì— ë„£ìœ¼ì‹œë©´ ë°”ë¡œ ì‹¤í–‰ ê°€ëŠ¥í•©ë‹ˆë‹¤.
---
### Question)
```
raw data ë¼ í•˜ë©´, timeKD ëª¨ë¸ì— ì§‘ì–´ë„£ì€ CSV íŒŒì¼ì„ ë§í•˜ëŠ”ê±°ì§€
```
### Answer)
ë„¤, ë§ìŠµë‹ˆë‹¤\! ì •í™•í•˜ê²Œ ì´í•´í•˜ì…¨ìŠµë‹ˆë‹¤.

í•˜ì§€ë§Œ ì¡°ê¸ˆ ë” êµ¬ì²´ì ìœ¼ë¡œ ë§ì”€ë“œë¦¬ë©´, \*\*"CSV íŒŒì¼ì— ìˆëŠ” ë°ì´í„°ë¥¼ ëª¨ë¸ì— ë„£ê¸° ìœ„í•´ ê°€ê³µí•œ(ì „ì²˜ë¦¬ëœ) ìƒíƒœ"\*\*ì—¬ì•¼ í•©ë‹ˆë‹¤.

TimeKD ëª¨ë¸ì´ ì„ë² ë”©ì„ ì¶”ì¶œí•  ë•Œ ì‚¬ìš©í–ˆë˜ **ì…ë ¥ ë°ì´í„°ì™€ ë˜‘ê°™ì€ í˜•íƒœ(Shape)ì™€ ìˆœì„œ**ë¥¼ ë§ì¶°ì£¼ëŠ” ê²ƒì´ í•µì‹¬ì…ë‹ˆë‹¤. ë‹¨ìˆœíˆ CSV íŒŒì¼ì„ `pd.read_csv`ë¡œ ì½ì€ ìƒíƒœ(Dataframe)ê°€ ì•„ë‹ˆë¼, **Windowing(ìŠ¬ë¼ì´ë”© ìœˆë„ìš°)** ë° **Normalization(ì •ê·œí™”)** ì²˜ë¦¬ê°€ ëë‚œ `Numpy Array` ë˜ëŠ” `Tensor` í˜•íƒœì—¬ì•¼ í•©ë‹ˆë‹¤.

ë‹¤ìŒ 3ê°€ì§€ë¥¼ ë°˜ë“œì‹œ ì²´í¬í•´ì£¼ì„¸ìš”:

### 1\. ì°¨ì›(Shape) ì¼ì¹˜ (ê°€ì¥ ì¤‘ìš”\!)

TimeKD ëª¨ë¸ì´ ì‹œê³„ì—´ì„ ì˜ë¼ì„œ(Windowing) ì²˜ë¦¬í–ˆë‹¤ë©´, `raw_data`ë„ ë˜‘ê°™ì´ ì˜ë¼ì ¸ ìˆì–´ì•¼ í•©ë‹ˆë‹¤.

  * **TimeKD ì„ë² ë”© ê°œìˆ˜:** $N$ê°œ (ì˜ˆ: 10,000ê°œ)
  * **Raw Data ê°œìˆ˜:** ì •í™•íˆ $N$ê°œì—¬ì•¼ í•©ë‹ˆë‹¤.
      * CSV íŒŒì¼ì€ ë³´í†µ `(ì „ì²´ ì‹œê°„, ë³€ìˆ˜)` í˜•íƒœì˜ ê¸´ 2ì°¨ì› í–‰ë ¬ì…ë‹ˆë‹¤.
      * TimeKDì— ë“¤ì–´ê°ˆ ë•Œ `(ìƒ˜í”Œ ìˆ˜ N, ìœˆë„ìš° ê¸¸ì´ T, ë³€ìˆ˜ ê°œìˆ˜ F)` í˜•íƒœì˜ 3ì°¨ì›ìœ¼ë¡œ ë³€í™˜ë˜ì—ˆì„ ê²ë‹ˆë‹¤.
      * **ì´ 3ì°¨ì› ë°°ì—´**ì´ ë°”ë¡œ ì œê°€ ë§ì”€ë“œë¦° `raw_data`ì…ë‹ˆë‹¤.

### 2\. ì •ê·œí™”(Scaling) ìƒíƒœ ì¼ì¹˜

TimeKD ëª¨ë¸ì´ í•™ìŠµë  ë•Œ ë°ì´í„°ë¥¼ 0\~1 ì‚¬ì´ë¡œ ì •ê·œí™”(MinMax Scaling)í•´ì„œ ë“¤ì–´ê°”ë‹¤ë©´, ë³µì›í•  ì •ë‹µì§€ì¸ `raw_data`ë„ **ë˜‘ê°™ì´ ì •ê·œí™”ë˜ì–´ ìˆì–´ì•¼ í•©ë‹ˆë‹¤.**

  * ë§Œì•½ ì •ê·œí™”ë˜ì§€ ì•Šì€ ì›ë˜ ê°’(ì˜ˆ: ì˜¨ë„ 25.5, ì „ë ¥ 1000 ë“±)ì„ íƒ€ê²Ÿìœ¼ë¡œ ë‘ë©´, ì„ë² ë”©ì€ ì •ê·œí™”ëœ íŠ¹ì§•ì„ ë‹´ê³  ìˆëŠ”ë° ë””ì½”ë”ëŠ” í° ê°’ì„ ì˜ˆì¸¡í•´ì•¼ í•´ì„œ í•™ìŠµì´ ì˜ ì•ˆ ë©ë‹ˆë‹¤.
  * **Imputationì´ ëë‚œ í›„**, ë‹¤ì‹œ ì—­ë³€í™˜(Inverse Scaling)ì„ í•´ì„œ ì›ë˜ ê°’ìœ¼ë¡œ ëŒë¦¬ë©´ ë©ë‹ˆë‹¤.

### 3\. ê²°ì¸¡ì¹˜(NaN) ìœ ì§€

TimeKDì— ë„£ì„ ë•ŒëŠ” ì—ëŸ¬ë¥¼ ë§‰ê¸° ìœ„í•´ 0ìœ¼ë¡œ ì±„ì›Œì„œ ë„£ì—ˆë”ë¼ë„, **ì´ë²ˆ í•™ìŠµì— ì‚¬ìš©í•  `raw_data`ì—ëŠ” ì›ë˜ì˜ ê²°ì¸¡ì¹˜(`NaN`)ê°€ ê·¸ëŒ€ë¡œ ë‚¨ì•„ ìˆì–´ì•¼ í•©ë‹ˆë‹¤.**

  * ê·¸ë˜ì•¼ ì•ì„œ ì½”ë“œì—ì„œ `mask = ~torch.isnan(target)` ë¶€ë¶„ì„ í†µí•´ \*\*"ì›ë˜ ê°’ì´ ìˆë˜ ë¶€ë¶„ë§Œ í•™ìŠµí•˜ê³ , ê²°ì¸¡ëœ ë¶€ë¶„ì€ ë¬´ì‹œí•´ë¼"\*\*ë¼ê³  ì•Œë ¤ì¤„ ìˆ˜ ìˆê¸° ë•Œë¬¸ì…ë‹ˆë‹¤.

-----

### ìš”ì•½ ê·¸ë¦¼

```text
[ CSV íŒŒì¼ (ì›ë³¸) ]
      â†“
[ ì „ì²˜ë¦¬ 1: Windowing (ì‹œê³„ì—´ ìë¥´ê¸°) ]  <-- ì„ë² ë”© ê°œìˆ˜ì™€ ë§ì¶°ì•¼ í•¨
      â†“
[ ì „ì²˜ë¦¬ 2: Normalization (ì •ê·œí™”) ]     <-- ëª¨ë¸ í•™ìŠµí™˜ê²½ê³¼ ë§ì¶°ì•¼ í•¨
      â†“
[ Raw Data (N, T, F) ] ================> [ Datasetì˜ targetìœ¼ë¡œ ì‚¬ìš© ]
      â”‚                                         (ì´ ë°ì´í„°ì—” NaNì´ í¬í•¨ë˜ì–´ ìˆì–´ì•¼ í•¨)
      â”‚
      â””---> [ TimeKD ëª¨ë¸ ] ---> [ .h5 íŒŒì¼ (ì„ë² ë”©) ] ===> [ Datasetì˜ inputìœ¼ë¡œ ì‚¬ìš© ]
```

ì¦‰, \*\*`.h5` íŒŒì¼ì„ ë§Œë“¤ ë•Œ ë„£ì—ˆë˜ ê·¸ ì…ë ¥ ë°ì´í„°(Numpy ë°°ì—´)\*\*ë¥¼ ê·¸ëŒ€ë¡œ ê°€ì ¸ì˜¤ì‹œë˜, `NaN` ê°’ë§Œ ì‚´ë ¤ë‘ì‹œë©´ ë©ë‹ˆë‹¤.
---
### Question)
```
ë‚˜ëŠ” timeKD ëª¨ë¸ì— CSV ë¥¼ ë„£ì—ˆì–´.
ê·¸ëŸ¬ë©´, timeKD ëª¨ë¸ì—ì„œ ë°ì´í„° ì „ì²˜ë¦¬í•˜ëŠ” ë¶€ë¶„ì„ ê°€ì ¸ì™€ì•¼í•˜ë‚˜?
í•´ë‹¹ ë¶€ë¶„ì˜ ì½”ë“œë¥¼ ì²¨ë¶€í• í…Œë‹ˆ ì–´ë–»ê²Œ í•˜ë©´ ì¢‹ì„ì§€ ì•Œë ¤ì¤˜. [ì½”ë“œ ì²¨ë¶€]
```
### Answer)
ë„¤, **ë¬´ì¡°ê±´ `data_loader_emb.py`ì— ìˆëŠ” ì „ì²˜ë¦¬ ë¡œì§ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•´ì•¼ í•©ë‹ˆë‹¤.**

ì´ìœ ëŠ” ì•„ì£¼ ì¤‘ìš”í•©ë‹ˆë‹¤:

1.  **ë°ì´í„° ì •ë ¬(Alignment):** TimeKD ëª¨ë¸ì´ ì„ë² ë”©ì„ ë§Œë“¤ ë•Œ ì‚¬ìš©í•œ `train/val/test`ì˜ \*\*ì‹œì‘ì (border)\*\*ê³¼ **ìœˆë„ìš° ìŠ¬ë¼ì´ì‹±(s\_begin:s\_end)** ê·œì¹™ì´ `data_loader_emb.py`ì— ì •ì˜ë˜ì–´ ìˆìŠµë‹ˆë‹¤. ì´ê²ƒì´ í‹€ì–´ì§€ë©´ \*\*ì„ë² ë”©(Input)\*\*ê³¼ \*\*ì •ë‹µ ì‹œê³„ì—´(Target)\*\*ì´ ì„œë¡œ ë‹¤ë¥¸ ì‹œê°„ì„ ê°€ë¦¬í‚¤ê²Œ ë˜ì–´ í•™ìŠµì´ ë¶ˆê°€ëŠ¥í•´ì§‘ë‹ˆë‹¤.
2.  **ìŠ¤ì¼€ì¼ë§(Scaling):** ì„ë² ë”©ì€ ì •ê·œí™”ëœ ë°ì´í„°(`StandardScaler`)ë¥¼ ë³´ê³  ë§Œë“¤ì–´ì¡ŒìŠµë‹ˆë‹¤. ë³µì›í•  ì •ë‹µì§€(`seq_x`)ë„ ë™ì¼í•œ ìŠ¤ì¼€ì¼ëŸ¬ë¡œ ì •ê·œí™”ë˜ì–´ ìˆì–´ì•¼ Denoising Autoencoderê°€ ì œëŒ€ë¡œ ì‘ë™í•©ë‹ˆë‹¤.

ì´ë¯¸ `data_loader_emb.py`ê°€ **ì›ë³¸ ë°ì´í„°ì™€ ì„ë² ë”©ì„ ê°™ì´ ë¡œë“œí•˜ë„ë¡** ì˜ ì§œì—¬ ìˆìŠµë‹ˆë‹¤. ì´ë¥¼ **ìƒì†(Inheritance)** ë°›ì•„ì„œ `Dataset`ì„ ì•„ì£¼ ì¡°ê¸ˆë§Œ ìˆ˜ì •í•˜ë©´ ë°”ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

### ìˆ˜ì • ê°€ì´ë“œ: `ImputationDataset` ë§Œë“¤ê¸°

ê¸°ì¡´ ì½”ë“œë¥¼ ê±´ë“œë¦¬ì§€ ì•Šê³ , ë³„ë„ íŒŒì¼(ì˜ˆ: `data_loader_impute.py` ë˜ëŠ” í•™ìŠµ ì½”ë“œ ë‚´ë¶€)ì—ì„œ ì•„ë˜ì™€ ê°™ì´ í´ë˜ìŠ¤ë¥¼ ìƒì†ë°›ì•„ ì‚¬ìš©í•˜ì„¸ìš”.

```python
import torch
import numpy as np
# data_loader_emb.py íŒŒì¼ì—ì„œ í•„ìš”í•œ ë°ì´í„°ì…‹ í´ë˜ìŠ¤ë¥¼ import í•˜ì„¸ìš”
from data_loader_emb import Dataset_ETT_hour, Dataset_Custom 

class ImputationDataset(Dataset_ETT_hour): # ë˜ëŠ” Dataset_Custom ë“± ì‚¬ìš©í•˜ëŠ” í´ë˜ìŠ¤ ìƒì†
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
    
    def __getitem__(self, index):
        # 1. ë¶€ëª¨ í´ë˜ìŠ¤ì˜ __getitem__ì„ í˜¸ì¶œí•´ì„œ ë°ì´í„°ì™€ ì„ë² ë”©ì„ ê°€ì ¸ì˜µë‹ˆë‹¤.
        # data_loader_emb.pyë¥¼ ë³´ë©´ ë¦¬í„´ê°’ì´ seq_x, seq_y, embeddings ì…ë‹ˆë‹¤.
        # Imputationì˜ ëª©í‘œëŠ” 'seq_x(ì…ë ¥ ìœˆë„ìš°)'ë¥¼ ë³µì›í•˜ëŠ” ê²ƒì´ë¯€ë¡œ seq_xê°€ Targetì´ ë©ë‹ˆë‹¤.
        seq_x, _, embeddings = super().__getitem__(index)
        
        # 2. ë°ì´í„° íƒ€ì… ë³€í™˜ (í•„ìš”ì‹œ)
        if isinstance(seq_x, np.ndarray):
            seq_x = torch.from_numpy(seq_x).float()
        
        # embeddingsê°€ í…ì„œê°€ ì•„ë‹ˆë©´ ë³€í™˜ (data_loader_emb.pyëŠ” ì´ë¯¸ í…ì„œë¡œ ì£¼ëŠ” ê²ƒìœ¼ë¡œ ë³´ì„)
        if not torch.is_tensor(embeddings):
            embeddings = torch.from_numpy(embeddings).float()

        # 3. Mask ìƒì„± (NaNì´ ìˆëŠ” ìœ„ì¹˜ ì°¾ê¸°)
        # ì£¼ì˜: ìŠ¤ì¼€ì¼ë§ ê³¼ì •ì—ì„œ NaNì´ ì–´ë–»ê²Œ ì²˜ë¦¬ë˜ì—ˆëŠ”ì§€ì— ë”°ë¼ ë‹¤ë¥¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
        # ì›ë³¸ CSVì— NaNì´ ìˆì—ˆë‹¤ë©´, seq_xì—ë„ NaNì´ ìˆì–´ì•¼ í•©ë‹ˆë‹¤.
        mask = ~torch.isnan(seq_x)
        
        # 4. NaNì„ 0.0ìœ¼ë¡œ ì¹˜í™˜ (ëª¨ë¸ì— ë„£ê¸° ìœ„í•´)
        target = torch.nan_to_num(seq_x, nan=0.0)
        
        # 5. DAE í•™ìŠµì— í•„ìš”í•œ (ì„ë² ë”©, ì •ë‹µë°ì´í„°, ë§ˆìŠ¤í¬) ë°˜í™˜
        return embeddings, target, mask
```

### ì£¼ì˜í•  ì  (StandardScalerì™€ ê²°ì¸¡ì¹˜)

`data_loader_emb.py`ì˜ `__read_data__`ë¥¼ ë³´ë©´ `StandardScaler`ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.

```python
if self.scale:
    train_data = df_data[border1s[0]:border2s[0]]
    self.scaler.fit(train_data.values)
    data = self.scaler.transform(df_data.values)
```

**ì¤‘ìš” ì²´í¬:** ë§Œì•½ CSV íŒŒì¼ ìì²´ì— ê²°ì¸¡ì¹˜(NaN)ê°€ í¬í•¨ë˜ì–´ ìˆë‹¤ë©´, ì¼ë°˜ì ì¸ `scaler.fit`ì„ í•˜ë©´ í‰ê· /ë¶„ì‚° ê³„ì‚° ì‹œ ì—ëŸ¬ê°€ ë‚˜ê±°ë‚˜ ê²°ê³¼ê°€ ì „ë¶€ NaNì´ ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

1.  **`utils/tools.py`ì˜ `StandardScaler` í™•ì¸:** ë‚´ë¶€ì—ì„œ `np.mean` ëŒ€ì‹  `np.nanmean`, `np.std` ëŒ€ì‹  `np.nanstd`ë¥¼ ì“°ëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”. ì•ˆ ì“´ë‹¤ë©´ ìˆ˜ì •í•´ì•¼ í•©ë‹ˆë‹¤.
2.  **ê°„ë‹¨í•œ í•´ê²°ì±…:** ìŠ¤ì¼€ì¼ëŸ¬ê°€ NaNì„ ì²˜ë¦¬ ëª» í•œë‹¤ë©´, `StandardScaler` ë¶€ë¶„ë§Œ ì•„ë˜ì²˜ëŸ¼ ìˆ˜ì •í•´ì„œ ì‚¬ìš©í•˜ì‹œëŠ” ê²Œ ì•ˆì „í•©ë‹ˆë‹¤ (ë°ì´í„° ë¡œë” ë‚´ë¶€ ìˆ˜ì • í•„ìš”).

<!-- end list -->

```python
# utils/tools.py í˜¹ì€ ë°ì´í„°ë¡œë” ë‚´ë¶€ ìˆ˜ì • ì˜ˆì‹œ
class StandardScaler:
    def fit(self, data):
        # nanmean, nanstdë¥¼ ì‚¬ìš©í•˜ì—¬ NaNì„ ë¬´ì‹œí•˜ê³  í†µê³„ëŸ‰ ê³„ì‚°
        self.mean = np.nanmean(data, axis=0)
        self.std = np.nanstd(data, axis=0)
        
    def transform(self, data):
        mean = torch.from_numpy(self.mean).type_as(data).to(data.device) if torch.is_tensor(data) else self.mean
        std = torch.from_numpy(self.std).type_as(data).to(data.device) if torch.is_tensor(data) else self.std
        return (data - mean) / std
```

### í•™ìŠµ ì½”ë“œì—ì„œì˜ ì‚¬ìš© ì˜ˆì‹œ

ì´ì œ ìœ„ì—ì„œ ë§Œë“  `ImputationDataset`ì„ ë¶ˆëŸ¬ì™€ì„œ í•™ìŠµí•˜ë©´ ë©ë‹ˆë‹¤.

```python
# í•™ìŠµ ì½”ë“œ
dataset = ImputationDataset(
    root_path='data',
    data_path='ETTh1.csv',
    flag='train',
    size=[24*4, 24*4, 24*4], # [seq_len, label_len, pred_len] (TimeKD ì„¤ì •ê³¼ ë™ì¼í•˜ê²Œ!)
    features='M',
    model_name='gpt2' # data_loader_embì— ìˆëŠ” ì¸ì
)

dataloader = DataLoader(dataset, batch_size=32, shuffle=True)

# ... í•™ìŠµ ë£¨í”„ ...
for embeddings, target, mask in dataloader:
    # embeddings: (Batch, Emb_Dim) -> Decoder ì…ë ¥
    # target: (Batch, Seq_Len, Feat) -> ì •ë‹µ
    # mask: (Batch, Seq_Len, Feat) -> Loss ê³„ì‚°ìš© ë§ˆìŠ¤í¬
    ...
```

**ìš”ì•½í•˜ìë©´:**

1.  ìƒˆë¡œ ì½”ë“œë¥¼ ì§œì§€ ë§ê³  **`data_loader_emb.py`ë¥¼ importí•´ì„œ ìƒì†**ë°›ìœ¼ì„¸ìš”.
2.  `__getitem__`ë§Œ ì˜¤ë²„ë¼ì´ë”©í•´ì„œ `seq_x`ì˜ **NaN ë§ˆìŠ¤í¬**ë¥¼ ìƒì„±í•´ì„œ ê°™ì´ ë¦¬í„´í•˜ê²Œ ë§Œë“œì„¸ìš”.
3.  `seq_len` ë“± ì‚¬ì´ì¦ˆ íŒŒë¼ë¯¸í„°ëŠ” **TimeKD í•™ìŠµ ë•Œì™€ ë˜‘ê°™ì´** ì„¤ì •í•´ì•¼ ì—ëŸ¬ê°€ ì•ˆ ë‚©ë‹ˆë‹¤.
---
### Question)
```
ì „ì²´ì ìœ¼ë¡œ ì½”ë“œë¥¼ ì •ë¦¬í•˜ë©´ ì–´ë–»ê²Œ ë˜ì§€?
í•˜ë‚˜ì˜ íŒŒì¼ë¡œ ë§Œë“¤ë©´ ì¢‹ê² ëŠ”ë°.
```
### Answer)
ë„¤, ìš”ì²­í•˜ì‹  ëŒ€ë¡œ \*\*ë°ì´í„° ë¡œë”©, ì „ì²˜ë¦¬, ëª¨ë¸ ì •ì˜, í•™ìŠµ, ê·¸ë¦¬ê³  ê²°ì¸¡ì¹˜ ë³´ê°„(Imputation)\*\*ê¹Œì§€ì˜ ëª¨ë“  ê³¼ì •ì„ \*\*í•˜ë‚˜ì˜ íŒŒì´ì¬ íŒŒì¼(`run_imputation.py`)\*\*ë¡œ ì •ë¦¬í•´ ë“œë¦¬ê² ìŠµë‹ˆë‹¤.

ê¸°ì¡´ `data_loader_emb.py`ì˜ í•µì‹¬ ë¡œì§(ë°ì´í„° ì •ë ¬ ë° h5 ë¡œë”©)ì„ í¬í•¨í•˜ë˜, **ê²°ì¸¡ì¹˜(NaN)ê°€ í¬í•¨ëœ ì›ë³¸ ë°ì´í„°ë¥¼ ì•ˆì „í•˜ê²Œ ì²˜ë¦¬í•˜ë„ë¡** ìˆ˜ì •í–ˆìŠµë‹ˆë‹¤.

ì´ ì½”ë“œë¥¼ ë³µì‚¬í•´ì„œ ë°”ë¡œ ì‹¤í–‰í•´ ë³´ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤. (ë‹¨, íŒŒì¼ ê²½ë¡œ ì„¤ì •ì€ ë³¸ì¸ í™˜ê²½ì— ë§ì¶°ì£¼ì„¸ìš”)

### `run_imputation.py`

```python
import os
import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import h5py
import warnings

warnings.filterwarnings('ignore')

# ==========================================
# 1. Utils (NaN ì²˜ë¦¬ê°€ ê°€ëŠ¥í•œ Scaler í¬í•¨)
# ==========================================
class RobustStandardScaler:
    """
    NaNì´ í¬í•¨ëœ ë°ì´í„°ì—ì„œë„ í‰ê· ê³¼ ë¶„ì‚°ì„ ê³„ì‚°í•  ìˆ˜ ìˆëŠ” Scaler
    """
    def __init__(self):
        self.mean = 0.
        self.std = 1.

    def fit(self, data):
        # np.mean ëŒ€ì‹  np.nanmeanì„ ì‚¬ìš©í•˜ì—¬ NaNì„ ë¬´ì‹œí•˜ê³  í†µê³„ëŸ‰ ê³„ì‚°
        self.mean = np.nanmean(data, axis=0)
        self.std = np.nanstd(data, axis=0)
        
        # í‘œì¤€í¸ì°¨ê°€ 0ì¸ ê²½ìš°(ëª¨ë“  ê°’ì´ ê°™ì€ ê²½ìš°) NaN ë°œìƒ ë°©ì§€
        self.std = np.where(self.std == 0, 1.0, self.std)

    def transform(self, data):
        # NaNì€ ê·¸ëŒ€ë¡œ ë‘” ì±„ ì •ê·œí™” ìˆ˜í–‰
        mean = torch.from_numpy(self.mean).type_as(data).to(data.device) if torch.is_tensor(data) else self.mean
        std = torch.from_numpy(self.std).type_as(data).to(data.device) if torch.is_tensor(data) else self.std
        return (data - mean) / std

    def inverse_transform(self, data):
        # ì •ê·œí™”ëœ ë°ì´í„°ë¥¼ ì›ë˜ ê°’ìœ¼ë¡œ ë³µì›
        mean = torch.from_numpy(self.mean).type_as(data).to(data.device) if torch.is_tensor(data) else self.mean
        std = torch.from_numpy(self.std).type_as(data).to(data.device) if torch.is_tensor(data) else self.std
        return (data * std) + mean

# ==========================================
# 2. Dataset (ê¸°ì¡´ ë¡œì§ + Imputation ì „ìš© ìˆ˜ì •)
# ==========================================
class ImputationDataset(Dataset):
    def __init__(self, root_path, data_path, flag='train', size=None, 
                 features='M', target='OT', scale=True, embed_subdir='emb_out'):
        """
        Args:
            root_path: ë°ì´í„° í´ë” ê²½ë¡œ
            data_path: CSV íŒŒì¼ ì´ë¦„
            embed_subdir: .h5 íŒŒì¼ë“¤ì´ ë“¤ì–´ìˆëŠ” í•˜ìœ„ í´ë” ê²½ë¡œ (timeKD ì¶œë ¥ êµ¬ì¡°ì— ë§ì¶¤)
        """
        # TimeKD ì„¤ì •ê³¼ ë™ì¼í•´ì•¼ í•¨
        self.seq_len = size[0]
        self.label_len = size[1]
        self.pred_len = size[2]
        
        assert flag in ['train', 'test', 'val']
        type_map = {'train':0, 'val':1, 'test':2}
        self.set_type = type_map[flag]
        
        self.features = features
        self.target = target
        self.scale = scale
        
        self.root_path = root_path
        self.data_path = data_path
        
        # .h5 íŒŒì¼ ê²½ë¡œ ì„¤ì • (ê¸°ì¡´ ì½”ë“œ ë¡œì§ ë°˜ì˜)
        # ì˜ˆ: data/ETTh1.csv/96/train/
        csv_name = data_path if data_path.endswith('.csv') else data_path + '.csv'
        self.full_csv_path = os.path.join(root_path, csv_name)
        
        # ì„ë² ë”© ê²½ë¡œ (ì‚¬ìš©ì í™˜ê²½ì— ë§ê²Œ ìˆ˜ì • í•„ìš”í•  ìˆ˜ ìˆìŒ)
        # ì—¬ê¸°ì„œëŠ” data_loader_emb.pyì˜ ë¡œì§ì„ ë”°ë¦„
        self.embed_path = os.path.join(root_path, csv_name, str(self.pred_len), flag)

        self.__read_data__()

    def __read_data__(self):
        self.scaler = RobustStandardScaler()
        df_raw = pd.read_csv(self.full_csv_path)

        # ë°ì´í„° ë¶„í•  ì§€ì  (ETT ë°ì´í„°ì…‹ ê¸°ì¤€ ì˜ˆì‹œ)
        # ë°ì´í„°ì…‹ ì¢…ë¥˜ì— ë”°ë¼ border ê°’ì´ ë‹¬ë¼ì§ˆ ìˆ˜ ìˆìœ¼ë‹ˆ í™•ì¸ í•„ìš”
        border1s = [0, 12*30*24 - self.seq_len, 12*30*24+4*30*24 - self.seq_len]
        border2s = [12*30*24, 12*30*24+4*30*24, 12*30*24+8*30*24]
        
        border1 = border1s[self.set_type]
        border2 = border2s[self.set_type]
        
        if self.features == 'M' or self.features == 'MS':
            cols_data = df_raw.columns[1:]
            df_data = df_raw[cols_data]
        elif self.features == 'S':
            df_data = df_raw[[self.target]]

        # ìŠ¤ì¼€ì¼ë§ (NaNì´ ìˆì–´ë„ ì•ˆì „í•˜ê²Œ ìˆ˜í–‰)
        if self.scale:
            train_data = df_data[border1s[0]:border2s[0]]
            self.scaler.fit(train_data.values)
            data = self.scaler.transform(df_data.values)
        else:
            data = df_data.values
            
        self.data_x = data[border1:border2]
        self.data_y = data[border1:border2]
    
    def __getitem__(self, index):
        s_begin = index
        s_end = s_begin + self.seq_len
        
        # 1. ì›ë³¸ ì‹œê³„ì—´ ë°ì´í„° (NaN í¬í•¨ ê°€ëŠ¥)
        seq_x = self.data_x[s_begin:s_end]
        
        # 2. ì„ë² ë”© ë¡œë“œ (.h5)
        file_path = os.path.join(self.embed_path, f"{index}.h5")
        
        if os.path.exists(file_path):
            with h5py.File(file_path, 'r') as hf:
                emb_data = hf['embeddings'][:] # í‚¤ ì´ë¦„ì´ 'embeddings'ì¸ì§€ í™•ì¸
                embedding = torch.from_numpy(emb_data).float()
                # ì°¨ì› ì •ë¦¬ (Batch ì°¨ì›ì´ ìˆë‹¤ë©´ ì œê±°)
                if embedding.dim() > 1 and embedding.shape[0] == 1:
                    embedding = embedding.squeeze(0)
        else:
            # íŒŒì¼ì´ ì—†ìœ¼ë©´ ì—ëŸ¬ í˜¹ì€ ë”ë¯¸ ë°˜í™˜ (í•™ìŠµ ë¬´ê²°ì„±ì„ ìœ„í•´ ì—ëŸ¬ ì¶”ì²œ)
            raise FileNotFoundError(f"Embedding file missing: {file_path}")

        # 3. ë°ì´í„° í…ì„œ ë³€í™˜ ë° ë§ˆìŠ¤í¬ ìƒì„±
        seq_x_tensor = torch.FloatTensor(seq_x)
        
        # Mask: ë°ì´í„°ê°€ ìˆìœ¼ë©´ 1, NaNì´ë©´ 0
        mask = ~torch.isnan(seq_x_tensor)
        
        # NaNì„ 0ìœ¼ë¡œ ì±„ì›€ (ëª¨ë¸ ì…ë ¥ìš©)
        target = torch.nan_to_num(seq_x_tensor, nan=0.0)

        # ë¦¬í„´: (ì„ë² ë”©, ì›ë³¸ê°’(0ì±„ì›€), ë§ˆìŠ¤í¬)
        return embedding, target, mask
    
    def __len__(self):
        return len(self.data_x) - self.seq_len - self.pred_len + 1

    def inverse_transform(self, data):
        return self.scaler.inverse_transform(data)

# ==========================================
# 3. Model (Decoder)
# ==========================================
class TimeDecoder(nn.Module):
    def __init__(self, emb_dim, seq_len, feat_dim):
        super(TimeDecoder, self).__init__()
        self.seq_len = seq_len
        self.feat_dim = feat_dim
        
        # ì„ë² ë”©(Flattened) -> ì‹œê³„ì—´ ë³µì›
        # ì„ë² ë”©ì´ (Seq, Dim) í˜•íƒœë¼ë©´ Flatten í•´ì„œ ë„£ê±°ë‚˜, 
        # TimeKD ì¶œë ¥ í˜•íƒœì— ë”°ë¼ êµ¬ì¡° ë³€ê²½ í•„ìš”. ì—¬ê¸°ì„  Flatten ê°€ì •.
        
        self.decoder = nn.Sequential(
            nn.Linear(emb_dim, 256),
            nn.ReLU(),
            nn.Linear(256, 512),
            nn.ReLU(),
            nn.Linear(512, seq_len * feat_dim)
        )
        
    def forward(self, z):
        # z shape: (Batch, Emb_Dim) í˜¹ì€ (Batch, Seq, Dim)
        # ë§Œì•½ ì…ë ¥ì´ (Batch, Seq, Dim)ì´ë©´ í¼ì³ì¤ë‹ˆë‹¤.
        if z.dim() == 3:
            z = z.reshape(z.size(0), -1)
            
        out = self.decoder(z)
        out = out.view(-1, self.seq_len, self.feat_dim)
        return out

# ==========================================
# 4. Main Execution
# ==========================================
if __name__ == '__main__':
    # --- ì„¤ì • (Configuration) ---
    args = {
        'root_path': './data/',      # ë°ì´í„° í´ë”
        'data_path': 'ETTh1.csv',    # CSV íŒŒì¼ëª…
        'seq_len': 96,               # TimeKD í•™ìŠµì‹œ seq_len
        'label_len': 48,             # TimeKD í•™ìŠµì‹œ label_len
        'pred_len': 96,              # TimeKD í•™ìŠµì‹œ pred_len
        'features': 'M',             # M: Multivariate, S: Univariate
        'emb_dim': 768 * 96,         # ***ì¤‘ìš”***: (ì„ë² ë”© ì°¨ì› x ì‹œí€€ìŠ¤ ê¸¸ì´) ë“± ì‹¤ì œ h5 ë°ì´í„° í¬ê¸°ì— ë§ê²Œ ì„¤ì •
        'feat_dim': 7,               # ë³€ìˆ˜ ê°œìˆ˜ (ETTh1ì€ 7ê°œ)
        'batch_size': 32,
        'epochs': 10,
        'lr': 0.001,
        'device': 'cuda' if torch.cuda.is_available() else 'cpu'
    }
    
    # 1. ë°ì´í„°ì…‹ ì¤€ë¹„
    dataset = ImputationDataset(
        root_path=args['root_path'],
        data_path=args['data_path'],
        flag='train',
        size=[args['seq_len'], args['label_len'], args['pred_len']],
        features=args['features']
    )
    dataloader = DataLoader(dataset, batch_size=args['batch_size'], shuffle=True)
    
    # 2. ëª¨ë¸ ì¤€ë¹„
    # ì£¼ì˜: emb_dimì€ ì‹¤ì œ h5 íŒŒì¼ì˜ shapeì„ ë³´ê³  (Seq * Dim) ì¸ì§€ í™•ì¸ í›„ ì„¤ì •í•´ì•¼ í•¨
    # ì²« ë²ˆì§¸ ë°°ì¹˜ë¥¼ ë¡œë“œí•´ì„œ ì°¨ì› í™•ì¸
    sample_emb, _, _ = next(iter(dataloader))
    input_dim = sample_emb.view(sample_emb.size(0), -1).shape[1]
    
    model = TimeDecoder(input_dim, args['seq_len'], args['feat_dim']).to(args['device'])
    optimizer = optim.Adam(model.parameters(), lr=args['lr'])
    criterion = nn.MSELoss(reduction='none') # Masked Lossë¥¼ ìœ„í•´ none ì„¤ì •

    print(f"Model Input Dim: {input_dim}, Output Shape: ({args['seq_len']}, {args['feat_dim']})")
    print("Start Training...")

    # 3. í•™ìŠµ ë£¨í”„
    for epoch in range(args['epochs']):
        model.train()
        train_loss = []
        
        for i, (emb, target, mask) in enumerate(dataloader):
            emb = emb.to(args['device'])
            target = target.to(args['device'])
            mask = mask.to(args['device'])
            
            # --- Denoising: ì„ë² ë”©ì— ë…¸ì´ì¦ˆ ì¶”ê°€ ---
            noise = torch.randn_like(emb) * 0.1
            noisy_emb = emb + noise
            
            # ì˜ˆì¸¡
            recon = model(noisy_emb)
            
            # Loss ê³„ì‚° (Maskê°€ 1ì¸ ë¶€ë¶„ë§Œ)
            loss = criterion(recon, target)
            loss = (loss * mask).sum() / (mask.sum() + 1e-8)
            
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            
            train_loss.append(loss.item())
            
        print(f"Epoch: {epoch+1}, Loss: {np.mean(train_loss):.5f}")

    print("Training Finished. Starting Imputation...")

    # 4. Imputation ë° ì €ì¥
    model.eval()
    imputed_results = []
    original_results = []
    
    with torch.no_grad():
        for i, (emb, target, mask) in enumerate(dataloader):
            emb = emb.to(args['device'])
            target = target.to(args['device'])
            mask = mask.to(args['device'])
            
            # ë…¸ì´ì¦ˆ ì—†ì´ ì˜ˆì¸¡
            recon = model(emb)
            
            # ê²°ì¸¡ì¹˜ ì±„ìš°ê¸°: (maskê°€ 1ì´ë©´ ì›ë³¸, 0ì´ë©´ ì˜ˆì¸¡ê°’)
            imputed = target * mask + recon * (1 - mask)
            
            # ì—­ë³€í™˜ (Inverse Scaling)ì„ ìœ„í•´ CPUë¡œ ì´ë™
            imputed = imputed.cpu().numpy()
            target = target.cpu().numpy() # ë¹„êµë¥¼ ìœ„í•´ ì›ë³¸ë„ ì €ì¥
            
            # ë°°ì¹˜ ë‚´ ê° ìƒ˜í”Œì— ëŒ€í•´ ì—­ë³€í™˜ ìˆ˜í–‰
            for k in range(imputed.shape[0]):
                inv_imputed = dataset.inverse_transform(imputed[k])
                imputed_results.append(inv_imputed)
                
    # ê²°ê³¼ í™•ì¸ (ì²« ë²ˆì§¸ ìƒ˜í”Œ)
    final_data = np.array(imputed_results)
    print(f"Imputation Complete. Final Data Shape: {final_data.shape}")
    
    # í•„ìš”ì‹œ ì €ì¥
    # np.save("imputed_data.npy", final_data)
```

### ğŸ’¡ ì‹¤í–‰ ì „ ì²´í¬ë¦¬ìŠ¤íŠ¸

1.  **ê²½ë¡œ í™•ì¸:** `args` ë”•ì…”ë„ˆë¦¬ì˜ `root_path`ì— CSV íŒŒì¼ì´ ìˆëŠ”ì§€, ê·¸ë¦¬ê³  TimeKDê°€ ìƒì„±í•œ ì„ë² ë”© í´ë” êµ¬ì¡°(`root/data_path/pred_len/flag/*.h5`)ê°€ ë§ëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.
2.  **`emb_dim` ìë™ ì„¤ì •:** ì½”ë“œëŠ” ì²« ë²ˆì§¸ ë°°ì¹˜ë¥¼ ì½ì–´ì„œ ì…ë ¥ ì°¨ì›ì„ ìë™ìœ¼ë¡œ ê³„ì‚°í•˜ë„ë¡ `input_dim` ë¡œì§ì„ ë„£ì–´ë‘ì—ˆìŠµë‹ˆë‹¤. ë³„ë„ë¡œ ì„¤ì •í•  í•„ìš” ì—†ìŠµë‹ˆë‹¤.
3.  **`feat_dim`:** ì‚¬ìš©í•˜ì‹œëŠ” ë°ì´í„°ì…‹ì˜ ë³€ìˆ˜ ê°œìˆ˜(columns - 1, date ì œì™¸)ì™€ ë§ëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”. (ETTh1ì€ 7ê°œ)

ì´ íŒŒì¼ í•˜ë‚˜ë§Œ ì‹¤í–‰í•˜ë©´ í•™ìŠµë¶€í„° ë³´ê°„ê¹Œì§€ ì™„ë£Œë©ë‹ˆë‹¤.