## Prompts (Powered by Gemini 2.5 Pro)
### Question)
```
임베딩 성능을 측정할거야.

내재적 평가 - 임베딩이 데이터를 잘 표현하는 정도를 어떻게 측정할 수 있을까?
```

### Answer)
네, 임베딩의 성능을 내재적 평가(Intrinsic Evaluation) 방식으로 측정하는 방법을 알려드릴게요.

내재적 평가는 임베딩 자체가 단어 간의 의미론적, 문법적 관계를 얼마나 잘 학습했는지, 즉 임베딩 공간(vector space)이 얼마나 잘 구성되었는지를 직접 평가하는 방식입니다. 특정 다운스트림 태스크(e.g., 감성 분석, 기계 번역)에 의존하지 않고 임베딩의 순수한 품질을 측정할 수 있어 빠르고 직관적입니다.

주요 내재적 평가 방법은 다음과 같습니다.

#### 🧐 주요 내재적 평가 방법
#### 1. 단어 유추 (Word Analogy)
가장 유명하고 직관적인 평가 방법입니다. 단어 벡터 간의 덧셈/뺄셈 연산을 통해 단어 간의 관계를 잘 유추하는지 확인합니다.

- 평가 내용: A와 B의 관계가 C와 D의 관계와 같은지를 평가합니다.

가장 고전적인 예시: king - man + woman = ? 이라는 벡터 연산을 했을 때, 결과 벡터가 queen 벡터와 얼마나 가까운지를 측정합니다.

문법적 관계: walking - walk + swim = swimming

의미론적 관계 (수도): Seoul - Korea + Japan = Tokyo

- 측정 방법:

A - B + C 와 같이 벡터 연산을 수행합니다.

결과로 나온 벡터와 코사인 유사도(Cosine Similarity)가 가장 높은 단어를 어휘 목록(vocabulary)에서 찾습니다.

찾아낸 단어가 정답(D)과 일치하는지 확인합니다.

- 주요 메트릭: 정확도 (Accuracy)

- 대표 데이터셋:

Google Analogy Dataset: 약 2만 개의 유추 질문(수도, 통화, 비교급 등)으로 구성됩니다.

BATS (Bigger Analogy Test Set): 더 크고 다양한 관계를 포함하는 데이터셋입니다.

#### 2. 단어 유사도 (Word Similarity)
사람이 평가한 단어 쌍의 유사도 점수와 임베딩 모델이 계산한 유사도 점수가 얼마나 비슷한지를 비교하여 평가합니다.

평가 내용: 의미적으로 유사한 단어들이 임베딩 공간에서 실제로 가깝게 배치되었는지를 평가합니다.

예시: (자동차, 트럭)은 (자동차, 꽃)보다 벡터 공간에서 더 가까워야 합니다.

측정 방법:

미리 정의된 단어 쌍 목록(e.g., (love, sex), (tiger, cat))에 대해 사람이 0~10점 척도로 유사도를 매깁니다. (이것이 정답 데이터셋입니다)

임베딩 모델에서 해당 단어 쌍의 벡터 간 코사인 유사도를 계산합니다.

사람이 매긴 점수 목록과 모델이 계산한 유사도 점수 목록 간의 **상관 관계(correlation)**를 계산합니다.

주요 메트릭: 스피어만 상관 계수(Spearman's rank correlation coefficient) 또는 피어슨 상관 계수(Pearson correlation coefficient). 이 값이 1에 가까울수록 임베딩이 사람의 판단과 유사하게 단어의 의미를 표현하고 있다는 뜻입니다.

- 대표 데이터셋:

WordSim-353: 353개의 단어 쌍으로 구성된 고전적인 데이터셋입니다.

SimLex-999: 진정한 의미적 유사도(synonymy)에 더 초점을 맞춘 데이터셋입니다.

#### 3. 이상치 탐지 (Outlier Detection / Word Intrusion)
주어진 단어 목록에서 의미적으로 가장 관련이 없는 단어를 찾아내는 능력을 평가합니다.

- 평가 내용: "사과, 바나나, 오렌지, 자동차" 목록이 주어졌을 때, 임베딩 모델이 '자동차'를 이상치(outlier)로 식별할 수 있는지 확인합니다.

- 측정 방법:

목록에 있는 모든 단어 벡터들의 평균 벡터(중심점, centroid)를 계산합니다.

각 단어 벡터와 평균 벡터 사이의 거리가 가장 먼 단어를 이상치로 선택합니다.

이 선택이 정답과 일치하는지 확인합니다.

- 주요 메트릭: 정확도 (Accuracy)
  
----
### Question)
```
주어지는 데이터셋은 시계열 데이터야. 그래도 이 방법을 적용할 수 있나?
```

### Answer)
아니요, 말씀드린 단어 유추, 단어 유사도 같은 방법들을 시계열 데이터에 직접 적용할 수는 없습니다.

그 방법들은 '단어'라는 명확하고 분리된 단위가 있는 자연어 처리(NLP)를 위해 설계되었기 때문입니다. 시계열 데이터는 연속적인 값의 흐름이라 '단어'와 같은 개별 단위가 없고, 'king - man + woman = queen'과 같은 관계가 존재하지 않습니다.

하지만, 시계열 데이터의 특성에 맞게 개념을 응용하고 변형하여 임베딩의 품질을 내재적으로 평가할 수는 있습니다.

📈 시계열 데이터에 내재적 평가를 적용하는 방법
핵심 아이디어는 시계열의 특정 **패턴(subsequence)**을 하나의 '단어'처럼 취급하여, 이 패턴들의 관계를 평가하는 것입니다.

1. 시계열 패턴의 유사도 평가 (Similarity Evaluation)
가장 실용적이고 중요한 방법입니다. 유사한 시계열 패턴이 임베딩 공간에서도 가깝게 위치하는지를 평가합니다.

평가 내용: 사람이 보거나 알고리즘으로 판단했을 때 유사한 두 개의 시계열 조각(window)이 임베딩 벡터 간 코사인 유사도도 높게 나타나는지를 확인합니다.

예: 주식 차트에서 '상승 시작' 패턴과 다른 시점의 '상승 시작' 패턴은 서로 유사한 벡터를 가져야 합니다.

측정 방법:

'정답' 유사도 정의: 전통적인 시계열 유사도 측정 알고리즘인 DTW(Dynamic Time Warping) 등을 사용하여 시계열 조각 쌍의 유사도를 미리 계산해 둡니다. 이것이 '정답' 점수가 됩니다.

임베딩 유사도 계산: 해당 시계열 조각들을 임베딩한 벡터 쌍의 코사인 유사도를 계산합니다.

상관관계 분석: 1번의 DTW 점수와 2번의 코사인 유사도 점수 간의 **상관 계수(Correlation)**를 계산합니다. 상관성이 높을수록 임베딩이 시계열 패턴의 유사도를 잘 표현하는 것입니다.

2. 이상치 탐지 (Outlier Detection)
정상적인 패턴들과 다른, 비정상적이거나 희귀한 패턴이 임베딩 공간에서 멀리 떨어져 있는지를 평가합니다.

평가 내용: 공장 설비의 정상 작동 시그널과 고장 시그널이 있을 때, 고장 시그널의 임베딩 벡터가 정상 시그널 벡터들의 분포에서 벗어나 있는지를 확인합니다.

측정 방법:

정상 패턴 데이터들을 임베딩하여 벡터 공간에 분포시킵니다.

비정상(Anamoly) 패턴 데이터를 임베딩합니다.

비정상 패턴의 벡터가 정상 패턴 벡터들의 중심(centroid)에서 얼마나 멀리 떨어져 있는지, 혹은 정상 패턴의 군집에 속하지 않는지를 측정합니다.

주요 메트릭: AUC, F1-Score 등으로 이상치 탐지 성능을 평가할 수 있습니다.

요약하자면, 시계열 데이터 임베딩의 내재적 평가는 패턴 간의 유사도 보존과 이상치 탐지 능력을 중심으로 수행하는 것이 가장 효과적입니다. 하지만 최종적으로는 예측, 분류, 군집화 등 실제 적용하려는 과제(외재적 평가)의 성능으로 임베딩의 가치를 판단하는 것이 가장 중요합니다.

----
### Question)
```
내가 가지고 있는 정보는, 데이터셋 하나와 training 코드뿐이야. 데이터셋에서 임의 패턴을 추출하고, 추출된 패턴에 대해 DTW를 수행한 결과를 정답으로 사용하나?

그리고 모델을 통해 계산된 유사도를 정답과 비교하는건가?
```

### Answer)
네, 정확히 맞습니다! 💯

완벽하게 이해하셨습니다. 그 과정을 조금 더 명확하게 정리해 드릴게요.

## 1. '정답' 유사도 (Ground Truth)
대상: 원본 시계열 데이터셋

방법: DTW (Dynamic Time Warping) 알고리즘 사용

결과: (패턴 A, 패턴 B)가 형태적으로 얼마나 유사한지에 대한 객관적인 수치 (거리 값). 이 값이 바로 **"정답"**이 됩니다.

## 2. '모델의 예측' 유사도 (Model's Prediction)
대상: 학습이 완료된 나의 임베딩 모델

방법:

'정답'을 만들 때 사용했던 똑같은 패턴 A와 패턴 B를 모델에 입력하여 임베딩 벡터를 각각 추출합니다.

두 벡터 사이의 코사인 유사도 (Cosine Similarity) 또는 **유클리드 거리 (Euclidean Distance)**를 계산합니다.

결과: 모델이 (패턴 A, 패턴 B)를 얼마나 유사하다고 "생각"하는지에 대한 **"예측 값"**이 됩니다.

## 최종 평가: 정답과 예측의 비교
이제 두 가지 결과가 준비되었습니다.

DTW 거리 리스트 (정답): [15.7, 22.1, 8.4, ...]

코사인 유사도 리스트 (예측): [0.92, 0.45, 0.98, ...]

마지막으로, 이 두 리스트가 얼마나 비슷한 경향을 보이는지를 **스피어만 상관 계수(Spearman's Correlation)**로 측정합니다.

상관 계수가 1에 가까울수록: DTW가 '유사하다'고 판단한 패턴들을 모델도 '유사하다'고 판단한 것이므로, 모델이 시계열 패턴의 형태적 유사성을 잘 학습했다고 결론 내릴 수 있습니다.

상관 계수가 0에 가깝거나 음수이면: 모델이 패턴의 유사성을 전혀 파악하지 못하고 있다는 의미입니다.

이처럼 자신의 데이터셋으로 직접 만든 '정답'을 기준으로, 모델의 성능을 내재적으로 평가하는 것이 이 방법의 핵심입니다.

----
### Question)
```
다음으로, 이상치 탐지는 어떻게 진행하지?
```

### Answer)
네, 이상치 탐지 평가 방법을 설명해 드릴게요.

이상치 탐지 평가는 정상 패턴들의 임베딩이 서로 모여 군집을 이루고, 비정상(이상치) 패턴의 임베딩은 그 군집에서 멀리 떨어져 있는지를 확인하는 과정입니다. 좋은 임베딩 모델이라면 정상과 이상의 구분이 뚜렷하게 나타나야 합니다.

가장 큰 전제 조건은 평가를 위해 어떤 패턴이 '정상'이고 어떤 패턴이 '이상치'인지 라벨(label)이 있는 데이터가 필요하다는 것입니다. 만약 라벨이 없다면, 도메인 지식을 활용해 명백한 이상 현상 몇 개를 직접 라벨링하여 평가 데이터셋을 만들어야 합니다.

## 👽 이상치 탐지 평가 단계
### 1단계: 데이터 준비
먼저, 라벨링된 데이터를 두 그룹으로 나눕니다.

정상(Normal) 데이터 그룹: 대다수를 차지하는 정상 상태의 시계열 조각(window)들입니다.

이상치(Outlier) 데이터 그룹: 드물게 발생하는 비정상 상태의 시계열 조각들입니다.

### 2단계: '정상' 데이터의 중심점(Centroid) 계산
학습된 임베딩 모델을 사용해 정상 데이터들이 벡터 공간에서 어디에 집중되어 있는지 그 중심을 찾습니다.

정상 데이터 임베딩: '정상 데이터 그룹'에 있는 모든 시계열 조각을 임베딩 모델에 통과시켜 벡터로 변환합니다.

중심점 계산: 변환된 모든 정상 벡터들의 평균 벡터를 계산합니다. 이것이 바로 '정상 군집'의 중심점, 즉 **센트로이드(Centroid)**가 됩니다.

### 3단계: 이상치 점수(Anomaly Score) 계산
이제 모든 데이터(정상, 이상치 모두)가 정상 군집의 중심에서 얼마나 멀리 떨어져 있는지 계산합니다. 이 거리가 바로 **'이상치 점수'**가 됩니다.

전체 데이터 임베딩: '정상 그룹'과 '이상치 그룹'의 모든 시계열 조각을 벡터로 변환합니다.

거리 계산: 각 데이터의 임베딩 벡터와 2단계에서 구한 센트로이드 벡터 사이의 거리를 계산합니다. 주로 **유클리드 거리(Euclidean Distance)**나 **코사인 거리(Cosine Distance)**를 사용합니다.

이상치 점수 = distance(해당 데이터의 임베딩 벡터, 정상 데이터의 센트로이드 벡터)

기대: 이상치 데이터는 이 점수가 높게 나오고, 정상 데이터는 낮게 나와야 합니다.

### 4단계: 성능 평가
마지막으로, '이상치 점수'를 사용해 모델이 정상과 이상치를 얼마나 잘 분류하는지 정량적으로 평가합니다.

방법: 모든 데이터 포인트에 대해 '실제 라벨(정상/이상치)'과 '이상치 점수'가 준비되었습니다. 이는 일반적인 이진 분류 문제의 성능 평가와 같습니다.

주요 메트릭:

AUC (Area Under the ROC Curve): 가장 추천하는 핵심 평가지표입니다. 특정 임계값(threshold)에 의존하지 않고, 모델이 전반적으로 이상치 데이터의 점수를 정상 데이터의 점수보다 얼마나 더 높게 잘 예측하는지를 종합적으로 보여줍니다. AUC가 1에 가까울수록 성능이 뛰어난 것입니다.

정밀도(Precision), 재현율(Recall), F1-Score: 특정 임계값을 기준으로 이상치라고 판단했을 때의 성능을 측정합니다. "이상치 점수가 0.8 이상이면 이상치로 분류한다"와 같은 규칙이 필요합니다.

## 요약
준비: '정상'과 '이상치'로 라벨링된 데이터를 준비합니다.

학습: 임베딩 모델을 학습시킵니다.

기준점 설정: '정상' 데이터들의 임베딩 벡터 평균(센트로이드)을 구합니다.

점수 계산: 모든 데이터에 대해 센트로이드로부터의 거리를 '이상치 점수'로 계산합니다.

성능 측정: 실제 라벨과 이상치 점수를 이용해 AUC로 최종 성능을 평가합니다.

결론적으로, 좋은 시계열 임베딩은 정상 패턴들을 벡터 공간의 한곳에 조밀하게 모아주고(낮은 분산), 이상치 패턴은 그곳에서 멀리 떨어뜨려 놓는 능력이 뛰어나야 합니다.

----
### Question)
```
임베딩 성능을 측정할거야.

외재적 평가 - 임베딩이 문제 해결에 유용한 정도를 어떻게 측정할 수 있을까?
```

### Answer)
네, 임베딩의 성능을 외재적 평가(Extrinsic Evaluation) 방식으로 측정하는 방법을 알려드릴게요.

외재적 평가는 임베딩이 실제 특정 문제를 얼마나 잘 해결하는지를 통해 임베딩의 유용성을 평가하는 방법입니다. 임베딩 자체의 품질(내재적 평가)보다는, 임베딩을 '재료'로 사용했을 때 최종 '요리'(애플리케이션)가 얼마나 훌륭한지를 보는 것과 같습니다.

이 때문에 외재적 평가는 임베딩의 실질적인 가치를 판단하는 **가장 확실한 '골드 스탠더드(Gold Standard)'**로 여겨집니다.

## 🚀 외재적 평가의 핵심 아이디어
외재적 평가는 **다운스트림 태스크(Downstream Task)**의 성능으로 임베딩의 성능을 간접적으로 측정합니다.

다운스트림 태스크 선택: 풀고자 하는 실제 문제(예: 스팸 메일 분류, 주가 예측, 사용자 그룹화)를 정합니다.

모델에 임베딩 적용: 선택한 문제를 푸는 모델(예: 분류 모델, 예측 모델)의 입력값(Feature)으로 우리가 만든 임베딩을 사용합니다.

성능 측정: 해당 모델이 얼마나 뛰어난 성능을 보이는지(예: 분류 정확도, 예측 오차)를 측정합니다. 이 성능이 바로 임베딩의 외재적 평가 점수가 됩니다.

## 📈 대표적인 다운스트림 태스크와 평가 방법
어떤 종류의 데이터에 임베딩을 적용했느냐에 따라 다양한 다운스트림 태스크를 선택할 수 있습니다.

### 1. 분류 (Classification)
가장 일반적이고 널리 사용되는 평가 방법입니다.

태스크 예시:

(자연어): 영화 리뷰를 '긍정/부정'으로 분류, 뉴스를 '스포츠/정치/경제' 카테고리로 분류

(시계열): 심전도(ECG) 파형을 보고 '정상/부정맥' 진단, 공장 설비 센서 데이터로 '정상/고장' 상태 분류

평가 방법:

각 데이터(문장, 시계열 조각)를 임베딩 벡터로 변환합니다.

이 임베딩 벡터를 입력으로 하고, 정답 라벨(긍정/부정, 정상/고장)을 출력으로 하는 간단한 분류 모델(예: 로지스틱 회귀, 서포트 벡터 머신, 간단한 신경망)을 학습시킵니다.

테스트 데이터셋에서 분류 모델의 정확도(Accuracy), F1-Score, 정밀도(Precision), 재현율(Recall) 등을 측정합니다. 이 점수가 높을수록 임베딩이 데이터의 핵심 특징을 잘 담아내고 있다는 뜻입니다.

### 2. 군집화 (Clustering)
라벨이 없는 데이터에서 임베딩이 의미 있는 그룹을 잘 형성하는지 평가합니다.

태스크 예시:

(자연어): 비슷한 주제의 뉴스 기사끼리 묶기

(시계열): 유사한 소비 패턴을 가진 고객 그룹 찾기, 비슷한 움직임을 보이는 주식 종목들 묶기

평가 방법:

모든 데이터를 임베딩 벡터로 변환합니다.

K-Means 같은 군집화 알고리즘을 사용해 임베딩 벡터들을 여러 그룹으로 나눕니다.

실루엣 스코어(Silhouette Score) 와 같은 군집화 평가지표를 사용해, 군집이 얼마나 잘 형성되었는지(같은 군집 내 데이터는 가깝고, 다른 군집 데이터와는 먼지)를 평가합니다.

### 3. 정보 검색 및 유사도 기반 추천 (Information Retrieval & Recommendation)
특정 데이터와 유사한 데이터를 얼마나 잘 찾아내는지 평가합니다.

태스크 예시:

(자연어): 특정 질문과 가장 유사한 질문 찾아주기

(시계열): 현재 주가 패턴과 과거에 가장 유사했던 패턴 찾아주기

평가 방법:

쿼리(질문, 현재 패턴)를 임베딩 벡터로 변환합니다.

전체 데이터베이스에 있는 모든 데이터를 임베딩 벡터로 미리 변환해 둡니다.

쿼리 벡터와 데이터베이스의 모든 벡터 간의 코사인 유사도를 계산하여 가장 유사한 Top-K개를 찾습니다.

찾아낸 결과가 실제로 얼마나 정확한지를 MAP(Mean Average Precision), NDCG 등의 정보 검색 평가지표로 측정합니다.

## 외재적 평가의 장단점
👍 장점:

가장 확실함: 임베딩이 실제 문제 해결에 얼마나 기여하는지를 직접적으로 보여줍니다.

최종 목표 지향적: "그래서 이 임베딩으로 무엇을 더 잘할 수 있는가?"라는 궁극적인 질문에 답을 줍니다.

👎 단점:

비용과 시간이 많이 듦: 다운스트림 태스크를 위한 별도의 모델을 만들고 학습시키는 과정이 필요합니다.

결과 해석의 복잡성: 최종 성능이 낮은 경우, 그것이 순수하게 임베딩의 문제인지, 아니면 다운스트림 모델 구조의 문제인지 분리해서 생각하기 어려울 수 있습니다.

따라서 일반적으로는 내재적 평가를 통해 여러 임베딩 모델 후보들을 빠르고 저렴하게 스크리닝한 후, 가능성이 높은 몇몇 모델에 대해서만 외재적 평가를 수행하여 최종 승자를 가리는 방식을 많이 사용합니다.

----
### Question)
```
다음으로,

일반화/효율성 평가 - 새로운 데이터, 적은 데이터를 대상으로 잘 작동하는 정도는 어떻게 측정하지
```

### Answer)
네, 임베딩의 일반화(Generalization) 및 효율성(Efficiency) 성능 평가 방법을 설명해 드릴게요.

이 평가는 임베딩이 단순히 학습 데이터만 잘 외운 것이 아니라, 처음 보는 데이터나 적은 양의 데이터에도 얼마나 잘 적용될 수 있는지, 즉 '진짜 실력'을 측정하는 과정입니다.

## 🧠 일반화 성능 평가 (새로운 데이터)
일반화 성능은 학습에 사용되지 않은, 처음 보는 데이터에 대해 모델이 얼마나 일관된 성능을 보이는지를 측정합니다. 성능 저하가 적을수록 일반화가 잘 된 것입니다.

#### 방법: 교차 검증 (Cross-Validation)
가장 표준적이고 신뢰도 높은 방법은 **K-겹 교차 검증(K-Fold Cross-Validation)**입니다.

데이터 분할: 다운스트림 태스크(예: 분류)를 위한 전체 라벨링된 데이터셋을 K개의 그룹(fold)으로 나눕니다. (보통 K=5 또는 10)

반복 학습 및 평가:

첫 번째 그룹을 평가(test)용으로 사용하고, 나머지 K-1개 그룹을 학습(train)용으로 사용하여 모델을 학습시키고 성능을 측정합니다.

두 번째 그룹을 평가용으로, 나머지를 학습용으로 사용하여 다시 학습하고 성능을 측정합니다.

이 과정을 K번 반복합니다.

최종 성능: K번 측정된 성능 점수들의 평균과 표준편차를 계산합니다. 평균 점수가 바로 그 모델의 일반화된 성능을 나타냅니다.

핵심 지표: 학습 데이터에서의 성능과 평가 데이터에서의 성능 차이. 이 차이가 크면 모델이 학습 데이터에만 과적합(overfitting)된 것이고, 차이가 작으면 일반화 성능이 뛰어나다고 할 수 있습니다. K-Fold 교차 검증의 평균 점수는 이 일반화 성능을 안정적으로 추정해 줍니다.

## ⚡️ 효율성 평가 (적은 데이터)
효율성은 아주 적은 양의 라벨링된 데이터만으로도 얼마나 빠르게 쓸만한 성능을 내는지를 측정합니다. 사전 학습된(Pre-trained) 임베딩의 가장 큰 장점 중 하나가 바로 이 효율성입니다.

#### 방법: 퓨샷 학습 (Few-Shot Learning)
"Shot"은 클래스(분류 대상) 당 학습 데이터의 개수를 의미합니다. 즉, 극소수의 예시만으로 학습하여 성능을 평가하는 것입니다.

초소형 학습셋 구성: 각 클래스별로 단 몇 개의 데이터(예: 5개)만 뽑아 학습(train) 데이터셋을 만듭니다. 이를 5-shot 학습이라고 부릅니다.

모델 학습: 이 초소형 데이터셋을 이용해 다운스트림 태스크 모델(예: 분류기)을 학습시킵니다. 임베딩이 데이터의 본질적 특징을 이미 잘 알고 있다면, 몇 개의 예시만으로도 충분히 학습할 수 있습니다.

성능 평가: 별도로 마련된 대량의 평가(test) 데이터셋으로 성능을 측정합니다.

핵심 지표: 매우 적은 학습 데이터(low-shot)로 달성한 정확도(Accuracy).

임베딩 A: 5-shot 학습으로 정확도 85% 달성

임베딩 B: 5-shot 학습으로 정확도 60% 달성

➡️ 결론: 임베딩 A가 훨씬 더 효율적이고 강력한, 즉 '알아듣는' 임베딩이라고 평가할 수 있습니다.

### (심화) 제로샷 학습 (Zero-Shot Learning)
가장 극한의 효율성 평가 방법입니다. 해당 클래스에 대한 학습 데이터를 단 하나도 보지 않고 문제를 맞히는 능력을 평가합니다. 이는 임베딩 공간 자체의 의미론적 구조가 매우 잘 형성되었을 때만 가능합니다.

실무에서는 K-Fold 교차 검증으로 안정적인 일반화 성능을 확인하고, 퓨샷 학습 시나리오를 통해 해당 임베딩이 얼마나 데이터 효율적인지(data-efficient)를 증명하는 방식으로 평가를 진행합니다.