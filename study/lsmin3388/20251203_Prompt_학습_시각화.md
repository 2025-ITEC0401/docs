# 학습 과정 시각화 학습 자료

## Prompts

### Question)
```
딥러닝 학습 과정에서 어떤 지표들을 모니터링해야 하나요? 각 지표가 의미하는 바는 무엇인가요?
```

### Answer)
**핵심 모니터링 지표:**

**1. Loss Curves**
```python
# Train Loss: 학습 데이터에 대한 손실
# Val Loss: 검증 데이터에 대한 손실

def plot_loss_curves(train_losses, val_losses):
    plt.figure(figsize=(10, 6))
    plt.plot(train_losses, label='Train Loss')
    plt.plot(val_losses, label='Validation Loss')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend()
    plt.title('Learning Curves')
    plt.show()
```

**해석:**
- Train↓, Val↓: 정상 학습
- Train↓, Val↑: 과적합
- Train→, Val→: 수렴 또는 학습 정체
- Train↑: 학습률 과다 또는 버그

**2. Learning Rate**
```python
# 스케줄러에 따른 학습률 변화 추적
lrs = []
for epoch in range(num_epochs):
    lrs.append(optimizer.param_groups[0]['lr'])
    scheduler.step()
```

**3. Gradient Statistics**
```python
def get_gradient_stats(model):
    total_norm = 0
    grad_means = []
    for p in model.parameters():
        if p.grad is not None:
            grad_means.append(p.grad.mean().item())
            total_norm += p.grad.norm(2).item() ** 2
    return {
        'grad_norm': total_norm ** 0.5,
        'grad_mean': np.mean(grad_means)
    }
```

**4. 태스크별 메트릭**

| 태스크 | 메트릭 |
|--------|--------|
| 분류 | Accuracy, F1, AUC |
| 예측 | MSE, MAE, MAPE |
| 클러스터링 | NMI, RI |

---

### Question)
```
TensorBoard나 Weights & Biases(wandb)를 사용하여 학습을 모니터링하는 방법을 알려주세요.
```

### Answer)
**TensorBoard 사용:**

```python
from torch.utils.tensorboard import SummaryWriter

writer = SummaryWriter('runs/experiment_1')

for epoch in range(num_epochs):
    train_loss = train_one_epoch()
    val_loss, val_acc = validate()

    # 스칼라 로깅
    writer.add_scalar('Loss/train', train_loss, epoch)
    writer.add_scalar('Loss/val', val_loss, epoch)
    writer.add_scalar('Accuracy/val', val_acc, epoch)

    # 학습률 로깅
    writer.add_scalar('Learning_rate',
                       optimizer.param_groups[0]['lr'], epoch)

    # 히스토그램 (가중치 분포)
    for name, param in model.named_parameters():
        writer.add_histogram(name, param, epoch)

writer.close()

# 실행: tensorboard --logdir=runs
```

**Weights & Biases 사용:**

```python
import wandb

# 초기화
wandb.init(
    project="time-series-project",
    config={
        "learning_rate": 0.001,
        "epochs": 100,
        "batch_size": 32,
        "model": "Transformer"
    }
)

for epoch in range(num_epochs):
    train_loss = train_one_epoch()
    val_loss, val_acc = validate()

    # 로깅
    wandb.log({
        "train_loss": train_loss,
        "val_loss": val_loss,
        "val_accuracy": val_acc,
        "epoch": epoch,
        "learning_rate": optimizer.param_groups[0]['lr']
    })

    # 모델 체크포인트 저장
    if val_loss < best_val_loss:
        wandb.save('best_model.pt')

wandb.finish()
```

**wandb 고급 기능:**

```python
# 이미지 로깅
wandb.log({"confusion_matrix": wandb.plot.confusion_matrix(
    probs=None, y_true=labels, preds=predictions)})

# 테이블 로깅
table = wandb.Table(columns=["input", "prediction", "target"])
for i in range(10):
    table.add_data(inputs[i], preds[i], targets[i])
wandb.log({"examples": table})

# Alert 설정
wandb.alert(
    title="Training complete",
    text=f"Final val_loss: {val_loss:.4f}"
)
```

---

### Question)
```
학습 과정에서 발생하는 문제들(과적합, 과소적합, 학습 불안정)을 시각화로 어떻게 진단하나요?
```

### Answer)
**1. 과적합 진단**

```python
def diagnose_overfitting(train_losses, val_losses):
    plt.figure(figsize=(12, 4))

    plt.subplot(1, 2, 1)
    plt.plot(train_losses, label='Train')
    plt.plot(val_losses, label='Validation')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend()
    plt.title('Loss Curves')

    # Gap 시각화
    plt.subplot(1, 2, 2)
    gap = np.array(val_losses) - np.array(train_losses)
    plt.plot(gap)
    plt.axhline(y=0, color='r', linestyle='--')
    plt.xlabel('Epoch')
    plt.ylabel('Val - Train Loss')
    plt.title('Generalization Gap')
    plt.show()

# Gap이 지속적으로 증가 → 과적합
```

**2. 학습 불안정 진단**

```python
def diagnose_instability(losses, gradients):
    fig, axes = plt.subplots(1, 3, figsize=(15, 4))

    # Loss 변동
    axes[0].plot(losses)
    axes[0].set_title('Loss (Spikes 확인)')

    # Gradient Norm
    axes[1].plot(gradients)
    axes[1].set_title('Gradient Norm')
    axes[1].axhline(y=1.0, color='r', linestyle='--')

    # Loss 변화율
    loss_diff = np.diff(losses)
    axes[2].plot(loss_diff)
    axes[2].set_title('Loss Change per Step')

    plt.tight_layout()
    plt.show()

# Gradient 폭발: norm이 갑자기 급증
# 학습 불안정: loss spike 빈번
```

**3. 수렴 문제 진단**

```python
def diagnose_convergence(train_losses, val_losses, window=10):
    # 이동 평균으로 트렌드 확인
    train_ma = np.convolve(train_losses,
                           np.ones(window)/window, mode='valid')
    val_ma = np.convolve(val_losses,
                         np.ones(window)/window, mode='valid')

    plt.figure(figsize=(10, 5))
    plt.plot(train_ma, label='Train MA')
    plt.plot(val_ma, label='Val MA')

    # 기울기 계산
    train_slope = np.gradient(train_ma)[-10:].mean()
    val_slope = np.gradient(val_ma)[-10:].mean()

    plt.title(f'Train slope: {train_slope:.6f}, Val slope: {val_slope:.6f}')
    plt.legend()
    plt.show()

    # slope ≈ 0: 수렴
    # slope > 0: 학습률 너무 높음
    # slope < 0 but slow: 학습률 너무 낮음
```

**문제별 해결 가이드:**

| 증상 | 진단 | 해결책 |
|------|------|--------|
| Val↑, Train↓ | 과적합 | Dropout, 정규화, 데이터 증강 |
| 둘 다 높음 | 과소적합 | 모델 복잡도↑, 학습 오래 |
| Loss spike | 불안정 | LR↓, Gradient clipping |
| 수렴 안함 | 최적화 문제 | LR 조정, 옵티마이저 변경 |
