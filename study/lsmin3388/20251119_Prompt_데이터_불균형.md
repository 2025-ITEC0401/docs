# 데이터 불균형 처리 학습 자료

## Prompts

### Question)
```
클래스 불균형(Class Imbalance) 문제란 무엇이고, 딥러닝 학습에 어떤 영향을 미치나요?
```

### Answer)
**클래스 불균형 정의**

데이터셋에서 특정 클래스의 샘플 수가 다른 클래스에 비해 현저히 많거나 적은 상황입니다.

**예시:**
- 이상 탐지: 정상 99%, 이상 1%
- 질병 진단: 건강 95%, 질병 5%
- 사기 탐지: 정상 거래 99.9%, 사기 0.1%

**학습에 미치는 영향:**

1. **다수 클래스 편향**: 모델이 다수 클래스만 예측하는 경향
2. **소수 클래스 무시**: 중요한 소수 클래스 패턴 학습 실패
3. **오해의 소지가 있는 정확도**: 99% 정확도지만 실제론 무의미

```python
# 불균형 데이터 예시
from collections import Counter
print(Counter(y_train))
# Counter({0: 9500, 1: 500})  # 19:1 불균형
```

**영향 시각화:**
```
정확도 역설:
- 모든 것을 "정상"으로 예측해도 정확도 99%
- 하지만 실제 이상 탐지 성능은 0%

Precision/Recall 관점:
- 높은 Precision, 낮은 Recall (또는 그 반대)
- F1 Score가 낮음
```

**평가 지표 권장:**
- Precision, Recall, F1-Score
- ROC-AUC, PR-AUC
- Confusion Matrix
- Cohen's Kappa

---

### Question)
```
클래스 불균형을 해결하기 위한 샘플링 기법(Over-sampling, Under-sampling, SMOTE)을 설명해주세요.
```

### Answer)
**1. Over-sampling (오버샘플링)**

소수 클래스의 샘플을 복제하여 데이터 균형을 맞춥니다.

```python
from imblearn.over_sampling import RandomOverSampler

ros = RandomOverSampler(random_state=42)
X_resampled, y_resampled = ros.fit_resample(X, y)
```

**장점:** 정보 손실 없음
**단점:** 과적합 위험, 중복 데이터

**2. Under-sampling (언더샘플링)**

다수 클래스의 샘플을 제거하여 균형을 맞춥니다.

```python
from imblearn.under_sampling import RandomUnderSampler

rus = RandomUnderSampler(random_state=42)
X_resampled, y_resampled = rus.fit_resample(X, y)
```

**장점:** 학습 속도 향상
**단점:** 정보 손실

**3. SMOTE (Synthetic Minority Over-sampling Technique)**

소수 클래스의 합성 샘플을 생성합니다.

```python
from imblearn.over_sampling import SMOTE

smote = SMOTE(random_state=42)
X_resampled, y_resampled = smote.fit_resample(X, y)
```

**동작 원리:**
1. 소수 클래스 샘플 선택
2. K-최근접 이웃 중 하나 선택
3. 두 샘플 사이 보간하여 새 샘플 생성

**비교:**

| 기법 | 데이터 양 | 과적합 위험 | 정보 손실 |
|------|----------|-----------|----------|
| Over-sampling | 증가 | 높음 | 없음 |
| Under-sampling | 감소 | 낮음 | 있음 |
| SMOTE | 증가 | 중간 | 없음 |

---

### Question)
```
모델 학습 단계에서 클래스 불균형을 처리하는 방법(Class Weight, Focal Loss 등)은 무엇인가요?
```

### Answer)
**1. Class Weight (클래스 가중치)**

손실 함수에서 소수 클래스에 더 높은 가중치를 부여합니다.

```python
from sklearn.utils.class_weight import compute_class_weight

# 자동 계산
class_weights = compute_class_weight(
    'balanced',
    classes=np.unique(y),
    y=y
)

# PyTorch CrossEntropyLoss에 적용
criterion = nn.CrossEntropyLoss(
    weight=torch.tensor(class_weights, dtype=torch.float)
)
```

**2. Focal Loss**

쉬운 샘플의 기여도를 줄이고 어려운 샘플에 집중합니다.

```python
class FocalLoss(nn.Module):
    def __init__(self, alpha=1, gamma=2):
        super().__init__()
        self.alpha = alpha
        self.gamma = gamma

    def forward(self, inputs, targets):
        ce_loss = F.cross_entropy(inputs, targets, reduction='none')
        pt = torch.exp(-ce_loss)
        focal_loss = self.alpha * (1 - pt) ** self.gamma * ce_loss
        return focal_loss.mean()

# gamma: 2~5 권장, 불균형이 심할수록 높게
```

**3. 샘플러 기반 접근**

배치 구성 시 클래스 균형 유지

```python
from torch.utils.data import WeightedRandomSampler

# 각 샘플의 가중치 계산
class_counts = np.bincount(y)
weights = 1. / class_counts[y]

sampler = WeightedRandomSampler(
    weights=weights,
    num_samples=len(weights),
    replacement=True
)

loader = DataLoader(dataset, batch_size=32, sampler=sampler)
```

**방법 비교:**

| 방법 | 구현 난이도 | 효과 | 권장 상황 |
|------|-----------|------|----------|
| Class Weight | 쉬움 | 보통 | 기본 시도 |
| Focal Loss | 중간 | 높음 | 심한 불균형 |
| WeightedSampler | 중간 | 높음 | 배치 균형 필요 |
| SMOTE + 위 방법 | 복잡 | 최고 | 극심한 불균형 |
