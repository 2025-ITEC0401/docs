# 배치 크기 학습 자료

## Prompts

### Question)
```
딥러닝에서 배치 크기(Batch Size)가 학습에 미치는 영향은 무엇인가요? 큰 배치와 작은 배치의 장단점을 비교해주세요.
```

### Answer)
**배치 크기의 영향:**

배치 크기는 한 번의 파라미터 업데이트에 사용되는 샘플 수를 결정합니다.

**작은 배치 (예: 16-64)**

**장점:**
- 더 자주 파라미터 업데이트 → 빠른 수렴
- 노이즈로 인한 정규화 효과
- 메모리 사용량 적음
- 일반화 성능 좋은 경향

**단점:**
- GPU 활용도 낮음
- 학습 시간 길어짐
- Gradient 추정 분산 높음

**큰 배치 (예: 512-4096)**

**장점:**
- GPU 병렬화 효율 높음
- 안정적인 gradient 추정
- 학습 시간 단축

**단점:**
- 메모리 사용량 많음
- 일반화 성능 저하 가능
- 수렴하는 최적점이 Sharp할 수 있음

```python
# 배치 크기에 따른 특성
small_batch = DataLoader(dataset, batch_size=32)
# 에포크당 업데이트: N/32번
# 노이즈 많지만 탐색적

large_batch = DataLoader(dataset, batch_size=512)
# 에포크당 업데이트: N/512번
# 안정적이지만 덜 탐색적
```

**비교:**

| 특성 | 작은 배치 | 큰 배치 |
|------|----------|---------|
| GPU 활용 | 낮음 | 높음 |
| 메모리 | 적음 | 많음 |
| 일반화 | 좋음 | 보통 |
| 수렴 속도 | 느림 | 빠름 |
| 학습 안정성 | 낮음 | 높음 |

---

### Question)
```
큰 배치로 학습할 때 learning rate를 어떻게 조정해야 하나요? Linear Scaling Rule이란 무엇인가요?
```

### Answer)
**Linear Scaling Rule:**

배치 크기를 k배 늘리면, 학습률도 k배 늘려야 합니다.

```python
# 기준: batch_size=256, lr=0.1

# 배치 크기 2배
batch_size = 512
lr = 0.2  # 학습률도 2배

# 배치 크기 4배
batch_size = 1024
lr = 0.4
```

**이론적 배경:**
```
SGD 업데이트: θ = θ - lr × (1/B) × Σ∇L_i

배치 크기 B가 2배 → gradient 합의 분산 1/2배
→ 학습률 2배로 동일한 업데이트 크기 유지
```

**Square Root Scaling (대안):**

```python
# 매우 큰 배치에서 linear scaling이 과할 때
lr_new = lr_base * sqrt(batch_size_new / batch_size_base)
```

**실무 적용:**

```python
class LargeMultiGPU:
    def __init__(self, base_batch=256, base_lr=0.1, n_gpus=8):
        self.effective_batch = base_batch * n_gpus
        self.lr = base_lr * n_gpus  # Linear scaling

        # Warmup 필수
        self.warmup_epochs = 5

def get_lr_schedule(base_lr, batch_size, base_batch=256):
    """배치 크기에 따른 학습률 조정"""
    scale = batch_size / base_batch
    scaled_lr = base_lr * scale

    return scaled_lr
```

**주의사항:**
- 매우 큰 배치(>8K)에서는 linear scaling 한계
- Warmup이 큰 배치에서 더 중요
- LARS, LAMB 등 특수 옵티마이저 고려

---

### Question)
```
메모리가 부족할 때 효과적인 배치 크기를 유지하는 방법(Gradient Accumulation)은 어떻게 구현하나요?
```

### Answer)
**Gradient Accumulation:**

여러 번의 forward-backward를 누적한 후 한 번에 업데이트합니다.

```python
# 목표 배치 크기: 256
# 실제 가능 배치 크기: 32
# Accumulation steps: 8 (32 × 8 = 256)

accumulation_steps = 8
optimizer.zero_grad()

for i, (inputs, targets) in enumerate(dataloader):
    outputs = model(inputs)
    loss = criterion(outputs, targets)

    # 스케일링: 평균을 맞추기 위해 나눔
    loss = loss / accumulation_steps
    loss.backward()

    if (i + 1) % accumulation_steps == 0:
        optimizer.step()
        optimizer.zero_grad()
```

**PyTorch Lightning 활용:**

```python
trainer = pl.Trainer(
    accumulate_grad_batches=8,  # 자동으로 gradient 누적
    precision=16  # 메모리 추가 절약
)
```

**주의사항:**

1. **Batch Normalization:**
```python
# BN 통계는 각 mini-batch마다 계산됨
# Accumulation과 무관하게 작은 배치 통계 사용
# → SyncBN 또는 GroupNorm 고려
```

2. **Gradient 스케일링:**
```python
# 반드시 accumulation_steps로 나누어야 함
loss = loss / accumulation_steps

# 또는 backward 후 스케일링
for param in model.parameters():
    if param.grad is not None:
        param.grad /= accumulation_steps
```

3. **Learning Rate 조정:**
```python
# Effective batch size 기준으로 LR 설정
effective_batch = batch_size * accumulation_steps
lr = base_lr * (effective_batch / base_batch)
```

**전체 구현:**

```python
class GradientAccumulationTrainer:
    def __init__(self, model, optimizer, accumulation_steps=8):
        self.model = model
        self.optimizer = optimizer
        self.accumulation_steps = accumulation_steps
        self.step_count = 0

    def train_step(self, batch):
        inputs, targets = batch
        outputs = self.model(inputs)
        loss = self.criterion(outputs, targets) / self.accumulation_steps
        loss.backward()

        self.step_count += 1

        if self.step_count % self.accumulation_steps == 0:
            # Gradient clipping (선택적)
            torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)

            self.optimizer.step()
            self.optimizer.zero_grad()

        return loss.item() * self.accumulation_steps
```
