# 하이퍼파라미터 튜닝

## Prompts (Powered by Claude)

### Question)
```
머신러닝 및 딥러닝 모델의 하이퍼파라미터 튜닝 방법론인 Grid Search, Random Search, Bayesian Optimization의 차이점과 각각의 장단점을 설명해주세요. 어떤 상황에서 어떤 방법을 선택해야 하나요?
```

### Answer)
하이퍼파라미터 튜닝의 세 가지 주요 방법론은 다음과 같이 구분됩니다:

**1. Grid Search (격자 탐색)**
- 모든 하이퍼파라미터 조합을 체계적으로 탐색하는 전수조사 방법
- 장점:
  - 정해진 범위 내에서 최적값을 반드시 찾을 수 있음
  - 구현이 간단하고 이해하기 쉬움
  - 재현성이 높음
- 단점:
  - 하이퍼파라미터 개수가 증가하면 계산 비용이 기하급수적으로 증가 (차원의 저주)
  - 비효율적인 탐색으로 시간과 자원 낭비 가능
- 적합한 상황: 하이퍼파라미터가 2-3개 이하이고, 탐색 범위가 명확할 때

**2. Random Search (무작위 탐색)**
- 정의된 범위 내에서 무작위로 하이퍼파라미터 조합을 샘플링
- 장점:
  - Grid Search보다 효율적 (동일 시간 내 더 넓은 영역 탐색)
  - 중요한 하이퍼파라미터를 더 다양하게 시도
  - 구현이 간단함
- 단점:
  - 최적값 발견을 보장하지 않음
  - 이전 결과를 활용하지 않아 비효율적일 수 있음
- 적합한 상황: 하이퍼파라미터가 많거나, 어떤 파라미터가 중요한지 모를 때

**3. Bayesian Optimization (베이지안 최적화)**
- 이전 평가 결과를 기반으로 다음 탐색 위치를 확률적으로 결정
- Gaussian Process 등을 사용하여 목적 함수를 모델링
- 장점:
  - 적은 반복으로 최적값에 근접 가능
  - 이전 결과를 활용하여 효율적으로 탐색
  - 비싼 평가 비용을 가진 모델에 적합
- 단점:
  - 구현이 복잡하고 이해하기 어려움
  - 차원이 매우 높으면 성능이 저하됨
  - 추가적인 계산 오버헤드 발생
- 적합한 상황: 모델 학습 비용이 높고, 효율적인 탐색이 중요할 때

**선택 가이드:**
- 빠른 프로토타이핑: Random Search
- 정확한 최적화 (작은 탐색 공간): Grid Search
- 비용 효율적 최적화 (큰 탐색 공간): Bayesian Optimization

---

### Question)
```
딥러닝 모델 학습에서 중요한 주요 하이퍼파라미터인 Learning Rate와 Batch Size에 대해 설명해주세요. 각각이 모델 학습에 미치는 영향과 적절한 값을 설정하는 방법은 무엇인가요?
```

### Answer)
Learning Rate와 Batch Size는 딥러닝 모델의 성능과 학습 속도에 가장 큰 영향을 미치는 하이퍼파라미터입니다.

**Learning Rate (학습률)**

Learning Rate는 모델이 gradient descent 과정에서 가중치를 얼마나 크게 업데이트할지 결정합니다.

- 너무 높은 경우:
  - 학습이 불안정하고 발산할 수 있음
  - Loss가 진동하거나 증가
  - 최적값을 지나쳐버림 (overshooting)

- 너무 낮은 경우:
  - 학습 속도가 매우 느림
  - Local minimum에 갇힐 가능성 증가
  - 수렴까지 많은 epoch 필요

- 적절한 값 설정 방법:
  - Learning Rate Finder: 작은 값부터 시작해 점진적으로 증가시키며 loss 변화 관찰
  - 일반적인 시작값: 0.001 (Adam), 0.01 (SGD)
  - Learning Rate Scheduling: Warmup, Step Decay, Cosine Annealing 등 활용
  - AdamW, RAdam 등 adaptive optimizer 사용

**Batch Size (배치 크기)**

Batch Size는 한 번의 gradient 업데이트에 사용되는 샘플 수를 의미합니다.

- 작은 Batch Size (예: 32, 64):
  - 장점:
    - 메모리 사용량 적음
    - Regularization 효과 (노이즈가 많은 gradient)
    - 더 나은 일반화 성능 가능
  - 단점:
    - 학습이 불안정할 수 있음
    - GPU 활용도가 낮아 느릴 수 있음

- 큰 Batch Size (예: 256, 512):
  - 장점:
    - 안정적인 gradient 추정
    - GPU 병렬 처리로 빠른 학습
    - 학습 곡선이 부드러움
  - 단점:
    - 많은 메모리 필요
    - Generalization gap 발생 가능
    - Sharp minimum에 수렴 경향

- 적절한 값 설정 방법:
  - 하드웨어 제약 고려 (GPU 메모리)
  - 데이터셋 크기 고려 (전체의 1-10% 수준)
  - Learning Rate와 함께 조정: Batch Size를 2배 늘리면 Learning Rate도 √2 ~ 2배 증가
  - 일반적인 범위: 32-256 사이

**실전 팁:**
- Learning Rate는 가장 먼저 튜닝해야 할 파라미터
- Batch Size는 하드웨어 허용 범위 내에서 크게 설정한 후, 필요시 줄이기
- 두 파라미터는 상호 의존적이므로 함께 조정 필요

---

### Question)
```
시계열 예측 모델(특히 Transformer 기반 모델)에서 중요한 하이퍼파라미터는 무엇이며, 어떻게 튜닝해야 하나요? PatchTST나 iTransformer 같은 최신 모델에서 고려해야 할 사항도 포함해주세요.
```

### Answer)
시계열 예측 모델, 특히 Transformer 기반 모델에서는 일반적인 하이퍼파라미터 외에도 시계열 데이터의 특성을 반영하는 파라미터들이 중요합니다.

**1. 시계열 특화 하이퍼파라미터**

**Look-back Window (Input Sequence Length)**
- 과거 데이터를 얼마나 볼지 결정
- 너무 짧으면: 패턴 학습 부족
- 너무 길면: 계산 비용 증가, 노이즈 포함 가능
- 설정 방법:
  - 데이터의 주기성 고려 (일별 데이터는 7일, 30일 등)
  - AutoCorrelation Function (ACF) 분석
  - 일반적으로 96~720 timesteps

**Prediction Horizon (Output Length)**
- 미래를 얼마나 예측할지 결정
- 비즈니스 요구사항에 따라 결정되는 경우가 많음
- Multi-step vs Auto-regressive 전략 선택

**2. Transformer 아키텍처 하이퍼파라미터**

**Model Dimension (d_model)**
- Transformer의 hidden dimension 크기
- 일반적인 범위: 128~512
- 데이터셋 크기와 복잡도에 따라 조정

**Number of Layers**
- Encoder/Decoder 레이어 개수
- 시계열 모델은 일반적으로 1~3층으로 충분 (NLP보다 얕음)
- 깊을수록 복잡한 패턴 학습 가능하지만 overfitting 위험

**Number of Attention Heads**
- Multi-head attention의 head 개수
- 일반적으로 4~8개
- d_model과 나누어떨어져야 함

**Feed-Forward Dimension**
- FFN의 hidden dimension
- 일반적으로 d_model의 2~4배

**3. PatchTST 특화 하이퍼파라미터**

**Patch Length**
- 시계열을 나누는 patch 크기
- 핵심적인 하이퍼파라미터
- 일반적인 범위: 8~32
- 데이터의 로컬 패턴을 고려하여 설정
- 너무 작으면: 패턴 손실, 계산 비용 증가
- 너무 크면: 세밀한 변화 캡처 실패

**Stride**
- Patch 간 이동 간격
- stride < patch_length: overlapping patches (더 많은 정보)
- stride = patch_length: non-overlapping (효율적)

**4. iTransformer 특화 하이퍼파라미터**

**Inverted Structure**
- 변수(variate)를 token으로, 시간을 embedding으로 처리
- 다변량 시계열에서 변수 간 관계 학습에 중요
- 변수 개수가 많을 때 특히 효과적

**5. 정규화 및 학습 하이퍼파라미터**

**Dropout Rate**
- 시계열 모델은 0.1~0.3이 일반적
- Attention dropout과 FFN dropout 별도 설정 가능

**Learning Rate Schedule**
- Warmup steps: 1000~5000
- Cosine annealing 효과적
- 초기 학습률: 1e-4 ~ 1e-3

**Normalization 방식**
- RevIN (Reversible Instance Normalization): 시계열에 매우 효과적
- Layer Normalization vs Batch Normalization

**6. 튜닝 전략**

**우선순위가 높은 파라미터 (순서대로):**
1. Learning Rate
2. Look-back Window / Prediction Horizon
3. Patch Length (PatchTST의 경우)
4. d_model, Number of Layers
5. Batch Size
6. Dropout Rate

**추천 튜닝 접근법:**
1. 기본값으로 베이스라인 구축
2. Learning Rate Finder로 적절한 학습률 찾기
3. Look-back window를 데이터 주기성에 맞춰 설정
4. Random Search로 주요 파라미터 탐색
5. 상위 3~5개 조합에 대해 Bayesian Optimization 적용
6. 최종 모델로 앙상블 고려

**실전 팁:**
- 시계열 모델은 데이터셋마다 최적 파라미터가 크게 다름
- Cross-validation 시 시간 순서 유지 (TimeSeriesSplit)
- 계절성, 트렌드 등 도메인 지식 활용
- 실험 추적 도구 사용 (Weights & Biases, MLflow)
