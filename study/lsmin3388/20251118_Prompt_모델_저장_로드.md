# 모델 저장 및 로드 학습 자료

## Prompts

### Question)
```
PyTorch에서 모델을 저장하고 로드하는 방법들(state_dict vs 전체 모델)의 차이점은 무엇인가요? 각각 언제 사용하나요?
```

### Answer)
**1. state_dict 저장 (권장)**

모델의 학습 가능한 파라미터(가중치, 편향)만 저장합니다.

```python
# 저장
torch.save(model.state_dict(), 'model_weights.pth')

# 로드
model = MyModel()  # 모델 구조 재정의 필요
model.load_state_dict(torch.load('model_weights.pth'))
```

**장점:**
- 파일 크기 작음
- Python 버전 독립적
- 유연한 로딩 (구조 변경 가능)

**2. 전체 모델 저장**

모델 구조와 파라미터를 함께 저장합니다.

```python
# 저장
torch.save(model, 'full_model.pth')

# 로드
model = torch.load('full_model.pth')
```

**장점:**
- 모델 클래스 정의 없이 로드 가능
- 간편함

**단점:**
- 파일 크기 큼
- Python/PyTorch 버전 의존성
- 클래스 정의 파일 경로 의존성

**권장 사용 상황:**

| 상황 | 권장 방식 |
|------|----------|
| 학습 체크포인트 | state_dict |
| 배포용 모델 | state_dict 또는 TorchScript |
| 빠른 프로토타이핑 | 전체 모델 |
| 장기 보관 | state_dict |

---

### Question)
```
학습 중 체크포인트를 저장할 때 모델 외에 어떤 정보들을 함께 저장해야 하나요?
```

### Answer)
**완전한 체크포인트 구성요소:**

1. **모델 가중치**: model.state_dict()
2. **옵티마이저 상태**: optimizer.state_dict()
3. **스케줄러 상태**: scheduler.state_dict()
4. **현재 에포크**: epoch
5. **손실 값**: loss
6. **랜덤 시드 상태**: torch.get_rng_state()

```python
# 체크포인트 저장
checkpoint = {
    'epoch': epoch,
    'model_state_dict': model.state_dict(),
    'optimizer_state_dict': optimizer.state_dict(),
    'scheduler_state_dict': scheduler.state_dict(),
    'loss': loss,
    'best_val_loss': best_val_loss,
    'rng_state': torch.get_rng_state(),
    'config': config  # 하이퍼파라미터
}
torch.save(checkpoint, f'checkpoint_epoch_{epoch}.pth')
```

**체크포인트 로드 및 학습 재개:**

```python
checkpoint = torch.load('checkpoint_epoch_50.pth')

model.load_state_dict(checkpoint['model_state_dict'])
optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
scheduler.load_state_dict(checkpoint['scheduler_state_dict'])

start_epoch = checkpoint['epoch'] + 1
best_val_loss = checkpoint['best_val_loss']
torch.set_rng_state(checkpoint['rng_state'])

# 학습 재개
for epoch in range(start_epoch, max_epochs):
    train_one_epoch()
```

**옵티마이저 상태가 중요한 이유:**
- Adam의 모멘텀 정보 유지
- Learning rate 스케줄 위치 복원
- 정확한 학습 재개 가능

---

### Question)
```
Multi-GPU로 학습한 모델을 Single GPU나 CPU에서 로드하려면 어떻게 해야 하나요?
```

### Answer)
**DataParallel로 학습한 모델:**

DataParallel은 모델을 'module.'로 감싸므로 키 이름이 변경됩니다.

```python
# DataParallel로 저장된 모델의 키
# 'module.layer1.weight' → 'layer1.weight'로 변환 필요

# 방법 1: 저장 시 module 제거
torch.save(model.module.state_dict(), 'model.pth')

# 방법 2: 로드 시 키 변환
state_dict = torch.load('model.pth')
new_state_dict = {}
for k, v in state_dict.items():
    name = k.replace('module.', '')
    new_state_dict[name] = v
model.load_state_dict(new_state_dict)
```

**GPU에서 CPU로 로드:**

```python
# GPU 모델을 CPU로 로드
model.load_state_dict(
    torch.load('model.pth', map_location=torch.device('cpu'))
)
```

**다른 GPU로 로드:**

```python
# GPU 0에서 GPU 1으로
model.load_state_dict(
    torch.load('model.pth', map_location={'cuda:0': 'cuda:1'})
)
```

**범용적인 로드 함수:**

```python
def load_model(model, path, device='cpu'):
    state_dict = torch.load(path, map_location=device)

    # DataParallel 키 처리
    if list(state_dict.keys())[0].startswith('module.'):
        state_dict = {k.replace('module.', ''): v
                      for k, v in state_dict.items()}

    model.load_state_dict(state_dict)
    return model.to(device)

# 사용
model = load_model(MyModel(), 'model.pth', device='cuda:0')
```

**DistributedDataParallel (DDP) 주의사항:**
- 저장 시 rank 0에서만 저장
- 모든 프로세스가 barrier 후 로드
