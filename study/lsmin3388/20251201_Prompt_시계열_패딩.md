# 시계열 패딩 학습 자료

## Prompts

### Question)
```
시계열 데이터에서 패딩(Padding)이 왜 필요한가요? 주요 패딩 방법들을 설명해주세요.
```

### Answer)
**패딩이 필요한 이유:**

1. **배치 처리**: 서로 다른 길이의 시퀀스를 동일한 배치로 처리
2. **모델 입력 요구사항**: 고정 길이 입력이 필요한 모델
3. **GPU 연산 효율**: 텐서 연산을 위한 균일한 차원

**주요 패딩 방법:**

**1. Zero Padding**
```python
def zero_padding(sequence, target_length):
    if len(sequence) >= target_length:
        return sequence[:target_length]
    padding = np.zeros(target_length - len(sequence))
    return np.concatenate([sequence, padding])

# [1, 2, 3] → [1, 2, 3, 0, 0, 0]
```

**2. Edge/Constant Padding**
```python
def edge_padding(sequence, target_length, mode='edge'):
    if len(sequence) >= target_length:
        return sequence[:target_length]
    pad_length = target_length - len(sequence)

    if mode == 'edge':  # 마지막 값으로 채움
        padding = np.full(pad_length, sequence[-1])
    else:  # 지정된 상수로 채움
        padding = np.full(pad_length, mode)

    return np.concatenate([sequence, padding])

# [1, 2, 3] → [1, 2, 3, 3, 3, 3]
```

**3. Reflect Padding**
```python
def reflect_padding(sequence, target_length):
    if len(sequence) >= target_length:
        return sequence[:target_length]

    padded = np.pad(sequence, (0, target_length - len(sequence)),
                    mode='reflect')
    return padded

# [1, 2, 3, 4] → [1, 2, 3, 4, 3, 2]
```

**비교:**

| 방법 | 장점 | 단점 | 권장 상황 |
|------|------|------|----------|
| Zero | 단순, 명확 | 분포 왜곡 | Mask와 함께 |
| Edge | 연속성 유지 | 상수 구간 | 예측 태스크 |
| Reflect | 패턴 유지 | 비현실적 | 신호 처리 |

---

### Question)
```
Attention 모델에서 패딩된 부분을 처리하기 위한 Padding Mask는 어떻게 구현하나요?
```

### Answer)
**Padding Mask의 필요성:**

패딩된 위치가 attention 계산에 영향을 주면 안 됩니다.

**Mask 구현:**

```python
def create_padding_mask(lengths, max_len):
    """
    lengths: 각 시퀀스의 실제 길이 리스트
    max_len: 패딩 후 최대 길이
    """
    batch_size = len(lengths)
    mask = torch.zeros(batch_size, max_len, dtype=torch.bool)

    for i, length in enumerate(lengths):
        mask[i, length:] = True  # 패딩 위치를 True로

    return mask

# 예: lengths=[3, 5], max_len=6
# mask = [[F, F, F, T, T, T],
#         [F, F, F, F, F, T]]
```

**Attention에 Mask 적용:**

```python
def masked_attention(Q, K, V, padding_mask=None):
    scores = Q @ K.transpose(-2, -1) / math.sqrt(Q.size(-1))

    if padding_mask is not None:
        # 패딩 위치의 score를 -inf로 설정
        # softmax 후 0이 됨
        scores = scores.masked_fill(
            padding_mask.unsqueeze(1).unsqueeze(2),
            float('-inf')
        )

    attention_weights = F.softmax(scores, dim=-1)
    return attention_weights @ V
```

**PyTorch MultiheadAttention 사용:**

```python
# PyTorch 내장 함수 활용
attention = nn.MultiheadAttention(d_model, num_heads)

# key_padding_mask: (batch, seq_len), True인 위치 무시
output, weights = attention(
    query, key, value,
    key_padding_mask=padding_mask
)
```

**주의사항:**
- Loss 계산 시에도 패딩 위치 제외 필요
- Encoder와 Decoder에서 별도 mask 관리

---

### Question)
```
가변 길이 시계열을 효율적으로 처리하는 방법(pack_padded_sequence, bucketing)은 무엇인가요?
```

### Answer)
**1. pack_padded_sequence (RNN용)**

```python
from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence

def process_variable_length(sequences, lengths):
    # 길이 내림차순 정렬 필요
    sorted_idx = sorted(range(len(lengths)),
                        key=lambda i: lengths[i], reverse=True)
    sorted_seqs = sequences[sorted_idx]
    sorted_lengths = [lengths[i] for i in sorted_idx]

    # 패킹
    packed = pack_padded_sequence(sorted_seqs, sorted_lengths,
                                   batch_first=True)

    # RNN 통과
    output, hidden = rnn(packed)

    # 언패킹
    unpacked, _ = pad_packed_sequence(output, batch_first=True)

    return unpacked
```

**장점:**
- 패딩에 대한 불필요한 계산 제거
- 메모리 효율적

**2. Bucketing (배치 구성 최적화)**

```python
class BucketBatchSampler:
    def __init__(self, lengths, batch_size, bucket_size=100):
        # 유사한 길이끼리 그룹화
        sorted_idx = np.argsort(lengths)

        # 버킷 단위로 배치 생성
        self.batches = []
        for i in range(0, len(sorted_idx), bucket_size):
            bucket = sorted_idx[i:i + bucket_size]
            np.random.shuffle(bucket)
            for j in range(0, len(bucket), batch_size):
                self.batches.append(bucket[j:j + batch_size])

    def __iter__(self):
        np.random.shuffle(self.batches)
        return iter(self.batches)
```

**3. Dynamic Batching (HuggingFace)**

```python
from transformers import DataCollatorWithPadding

# 배치 내 최대 길이로만 패딩
collator = DataCollatorWithPadding(tokenizer, padding=True)

# 각 배치마다 다른 최대 길이
# 전체 데이터의 최대 길이가 아닌 배치 내 최대 길이 사용
```

**방법 비교:**

| 방법 | 메모리 | 속도 | 구현 복잡도 |
|------|--------|------|-----------|
| 고정 패딩 | 낭비 | 빠름 | 낮음 |
| pack_padded | 효율적 | 중간 | 중간 |
| Bucketing | 효율적 | 빠름 | 중간 |
| Dynamic | 효율적 | 빠름 | 낮음 |

**Transformer에서의 권장:**

```python
class EfficientDataLoader:
    def collate_fn(self, batch):
        sequences = [item['sequence'] for item in batch]
        lengths = [len(s) for s in sequences]

        # 배치 내 최대 길이로 패딩
        max_len = max(lengths)
        padded = torch.zeros(len(batch), max_len)
        mask = torch.ones(len(batch), max_len, dtype=torch.bool)

        for i, seq in enumerate(sequences):
            padded[i, :len(seq)] = torch.tensor(seq)
            mask[i, :len(seq)] = False

        return padded, mask, lengths
```
