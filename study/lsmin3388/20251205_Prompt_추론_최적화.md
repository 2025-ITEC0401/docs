# 추론 최적화 학습 자료

## Prompts

### Question)
```
딥러닝 모델의 추론 속도를 최적화하는 방법들에는 어떤 것들이 있나요? 각각의 장단점은 무엇인가요?
```

### Answer)
**추론 최적화 기법 개요:**

**1. 모델 레벨 최적화**
- Quantization (양자화)
- Pruning (가지치기)
- Knowledge Distillation

**2. 그래프 레벨 최적화**
```python
# TorchScript로 그래프 최적화
scripted_model = torch.jit.script(model)
# 또는
traced_model = torch.jit.trace(model, example_input)

# 최적화 적용
optimized = torch.jit.optimize_for_inference(scripted_model)
```

**3. 런타임 최적화**
- ONNX Runtime
- TensorRT
- OpenVINO

**4. 배치 처리**
```python
# 동적 배치로 throughput 향상
batch_inputs = torch.cat(accumulated_inputs)
batch_outputs = model(batch_inputs)
```

**비교:**

| 방법 | 속도 향상 | 정확도 영향 | 구현 복잡도 |
|------|----------|-----------|-----------|
| TorchScript | 1.2-2x | 없음 | 낮음 |
| ONNX | 1.5-2x | 거의 없음 | 중간 |
| TensorRT | 2-5x | 약간 | 높음 |
| Quantization | 2-4x | 약간 | 중간 |

---

### Question)
```
TorchScript와 ONNX는 무엇이고, 어떻게 사용하나요?
```

### Answer)
**TorchScript:**

PyTorch 모델을 직렬화하고 최적화하는 방법입니다.

```python
# 방법 1: Scripting (동적 제어 흐름 지원)
@torch.jit.script
def my_function(x):
    if x.sum() > 0:
        return x * 2
    return x

scripted_model = torch.jit.script(model)

# 방법 2: Tracing (정적 그래프)
example_input = torch.randn(1, 10)
traced_model = torch.jit.trace(model, example_input)

# 저장 및 로드
traced_model.save('model.pt')
loaded_model = torch.jit.load('model.pt')

# C++에서도 사용 가능
```

**ONNX (Open Neural Network Exchange):**

프레임워크 간 모델 교환 형식입니다.

```python
import torch.onnx

# ONNX로 내보내기
dummy_input = torch.randn(1, 10)
torch.onnx.export(
    model,
    dummy_input,
    "model.onnx",
    input_names=['input'],
    output_names=['output'],
    dynamic_axes={'input': {0: 'batch_size'}}
)

# ONNX Runtime으로 추론
import onnxruntime as ort

session = ort.InferenceSession("model.onnx")
inputs = {'input': input_data.numpy()}
outputs = session.run(None, inputs)
```

**ONNX 최적화:**

```python
# ONNX 그래프 최적화
import onnx
from onnxruntime.transformers import optimizer

optimized_model = optimizer.optimize_model(
    "model.onnx",
    model_type='bert',
    num_heads=12,
    hidden_size=768
)
optimized_model.save_model_to_file("model_optimized.onnx")
```

**선택 가이드:**

| 상황 | 권장 |
|------|------|
| PyTorch 배포 | TorchScript |
| 다른 프레임워크 | ONNX |
| 최고 성능 필요 | TensorRT |
| 엣지 디바이스 | ONNX + TFLite |

---

### Question)
```
실시간 추론을 위한 최적화 전략과 벤치마킹 방법을 알려주세요.
```

### Answer)
**실시간 추론 최적화 전략:**

**1. 모델 워밍업**
```python
# 첫 추론은 느림 (CUDA 초기화 등)
def warmup(model, input_shape, n_runs=10):
    dummy_input = torch.randn(input_shape).cuda()
    with torch.no_grad():
        for _ in range(n_runs):
            _ = model(dummy_input)
    torch.cuda.synchronize()
```

**2. 추론 모드 설정**
```python
model.eval()  # Dropout, BN 비활성화

with torch.no_grad():  # Gradient 계산 비활성화
    with torch.cuda.amp.autocast():  # Mixed Precision
        output = model(input)
```

**3. 비동기 처리**
```python
# CUDA 스트림 활용
stream = torch.cuda.Stream()
with torch.cuda.stream(stream):
    output = model(input)

# 다른 작업 수행
# ...

stream.synchronize()  # 필요할 때 동기화
```

**벤치마킹:**

```python
import time

def benchmark(model, input_shape, n_runs=100, warmup_runs=10):
    device = next(model.parameters()).device
    dummy_input = torch.randn(input_shape).to(device)

    # Warmup
    with torch.no_grad():
        for _ in range(warmup_runs):
            _ = model(dummy_input)

    if device.type == 'cuda':
        torch.cuda.synchronize()

    # Benchmark
    times = []
    with torch.no_grad():
        for _ in range(n_runs):
            start = time.perf_counter()

            _ = model(dummy_input)

            if device.type == 'cuda':
                torch.cuda.synchronize()

            times.append(time.perf_counter() - start)

    return {
        'mean': np.mean(times) * 1000,
        'std': np.std(times) * 1000,
        'min': np.min(times) * 1000,
        'max': np.max(times) * 1000,
        'throughput': 1.0 / np.mean(times)
    }

# 사용
results = benchmark(model, (1, 100, 64))
print(f"Latency: {results['mean']:.2f} ± {results['std']:.2f} ms")
print(f"Throughput: {results['throughput']:.1f} samples/sec")
```

**프로파일링:**

```python
# PyTorch Profiler
from torch.profiler import profile, ProfilerActivity

with profile(
    activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA],
    record_shapes=True,
    profile_memory=True
) as prof:
    output = model(input)

print(prof.key_averages().table(sort_by="cuda_time_total"))
```

**최적화 체크리스트:**

| 단계 | 확인 사항 | 예상 효과 |
|------|----------|----------|
| 1 | model.eval() 설정 | 필수 |
| 2 | torch.no_grad() 사용 | 메모리 ↓ |
| 3 | Mixed Precision | 속도 1.5-2x |
| 4 | Batch 처리 | Throughput ↑ |
| 5 | TorchScript/ONNX | 속도 1.2-2x |
| 6 | TensorRT | 속도 2-5x |
