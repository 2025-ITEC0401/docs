# 과적합 방지 기법

## Prompts (Powered by Claude)

### Question)
```
딥러닝 모델에서 과적합을 방지하기 위한 Dropout 기법과 그 변형들에 대해 설명해주세요. 각 변형의 특징과 사용 시나리오도 포함해주세요.
```

### Answer)
Dropout은 훈련 과정에서 무작위로 뉴런을 비활성화하여 과적합을 방지하는 강력한 정규화 기법입니다.

**기본 Dropout**
- 훈련 시 각 뉴런을 확률 p로 무작위 제거
- 일반적으로 p=0.5를 사용 (50% 뉴런 비활성화)
- 뉴런 간 co-adaptation을 방지하여 더 강건한 특징 학습
- 테스트 시에는 모든 뉴런을 사용하되 출력에 (1-p)를 곱함

**Dropout 변형들**

1. **DropConnect**
   - 뉴런 대신 가중치를 무작위로 제거
   - 더 세밀한 정규화 가능
   - 계산 비용이 상대적으로 높음

2. **Spatial Dropout**
   - CNN에 특화된 방법
   - 개별 픽셀이 아닌 전체 feature map을 제거
   - 인접한 픽셀 간 상관관계가 높은 이미지 데이터에 효과적

3. **Variational Dropout**
   - RNN/LSTM에 적합
   - 시간 단계마다 동일한 dropout mask 사용
   - 시퀀스 데이터의 temporal 정보 보존

4. **Alpha Dropout**
   - SELU 활성화 함수와 함께 사용
   - Self-normalizing 특성 유지
   - 평균과 분산을 자동으로 조정

**사용 시나리오**
- 완전 연결층: 표준 Dropout (p=0.5)
- CNN: Spatial Dropout (p=0.2-0.3)
- RNN/LSTM: Variational Dropout (p=0.2-0.5)
- 작은 데이터셋: 더 높은 dropout rate
- 큰 데이터셋: 더 낮은 dropout rate

---

### Question)
```
Early Stopping 전략은 어떻게 작동하며, 실제 구현 시 고려해야 할 핵심 요소들은 무엇인가요?
```

### Answer)
Early Stopping은 검증 손실이 개선되지 않을 때 훈련을 조기 종료하여 과적합을 방지하는 기법입니다.

**작동 원리**
1. 훈련 데이터로 모델 학습
2. 매 에포크마다 검증 데이터로 성능 평가
3. 검증 손실이 최소값을 기록하면 모델 저장
4. 일정 에포크 동안 개선이 없으면 훈련 중단
5. 저장된 최적 모델 사용

**핵심 구현 요소**

1. **Patience (인내심)**
   - 성능 개선 없이 기다릴 에포크 수
   - 너무 작으면: 조기 종료로 학습 부족
   - 너무 크면: 과적합 위험 증가
   - 권장값: 10-50 (데이터셋 크기에 따라)

2. **Delta (최소 개선 임계값)**
   - 의미 있는 개선으로 인정할 최소 변화량
   - 노이즈로 인한 불필요한 업데이트 방지
   - 일반적으로 0.001-0.0001 사용

3. **모니터링 지표**
   - 검증 손실 (가장 일반적)
   - 검증 정확도
   - F1 Score, AUC 등 태스크 특화 지표
   - 여러 지표를 함께 모니터링 권장

4. **체크포인트 저장**
   - 최적 모델 가중치 저장
   - 여러 버전 유지 (상위 k개)
   - 메모리 효율적인 저장 전략

**구현 예시 (PyTorch)**
```python
class EarlyStopping:
    def __init__(self, patience=10, delta=0.001):
        self.patience = patience
        self.delta = delta
        self.counter = 0
        self.best_loss = None
        self.early_stop = False

    def __call__(self, val_loss, model):
        if self.best_loss is None:
            self.best_loss = val_loss
            self.save_checkpoint(model)
        elif val_loss > self.best_loss - self.delta:
            self.counter += 1
            if self.counter >= self.patience:
                self.early_stop = True
        else:
            self.best_loss = val_loss
            self.save_checkpoint(model)
            self.counter = 0
```

**실전 팁**
- 학습률 감소 스케줄러와 함께 사용
- 검증 데이터는 훈련에 사용하지 않음
- 최종 평가는 별도의 테스트 데이터로 수행
- Patience는 learning rate schedule과 조화롭게 설정

---

### Question)
```
L1과 L2 정규화의 수학적 원리와 차이점을 설명하고, 각각 어떤 상황에서 사용해야 하는지 알려주세요.
```

### Answer)
L1과 L2 정규화는 손실 함수에 가중치의 크기에 대한 페널티를 추가하여 과적합을 방지하는 기법입니다.

**수학적 정의**

**L2 정규화 (Ridge, Weight Decay)**
- 손실 함수: `L(w) = Loss(y, ŷ) + λ/2 * Σw²`
- 가중치의 제곱합을 페널티로 추가
- 가중치를 0에 가깝게 만들지만 0으로 만들지는 않음
- 미분 가능하고 볼록 함수

**L1 정규화 (Lasso)**
- 손실 함수: `L(w) = Loss(y, ŷ) + λ * Σ|w|`
- 가중치의 절댓값 합을 페널티로 추가
- 많은 가중치를 정확히 0으로 만듦 (희소성)
- 0에서 미분 불가능

**주요 차이점**

| 특성 | L1 정규화 | L2 정규화 |
|------|-----------|-----------|
| 가중치 분포 | 희소 (많은 0) | 작고 고르게 분산 |
| Feature Selection | 자동 수행 | 수행 안 함 |
| 계산 효율성 | 약간 느림 | 빠름 |
| 해석 가능성 | 높음 (중요 특성 식별) | 낮음 |
| 다중공선성 | 하나만 선택 | 고르게 분산 |

**기하학적 해석**
- L1: 다이아몬드 모양 제약 → 모서리에서 해 발견 (희소성)
- L2: 원형 제약 → 부드럽게 가중치 감소

**사용 시나리오**

**L1 정규화를 사용할 때:**
1. **Feature Selection이 필요한 경우**
   - 수백~수천 개의 특성 중 중요한 것만 선택
   - 해석 가능한 모델 필요

2. **고차원 희소 데이터**
   - 텍스트 분류 (많은 단어, 대부분 0)
   - 유전자 데이터 분석

3. **모델 압축**
   - 메모리 제약이 있는 환경
   - 추론 속도가 중요한 경우

**L2 정규화를 사용할 때:**
1. **다중공선성이 있는 경우**
   - 상관관계 높은 특성들을 모두 유지
   - 안정적인 가중치 학습

2. **딥러닝 모델**
   - 대부분의 신경망에서 표준
   - SGD와 잘 호환

3. **일반적인 과적합 방지**
   - 모든 특성이 중요한 경우
   - 부드러운 가중치 분포 선호

**Elastic Net (L1 + L2 결합)**
```python
Loss = MSE + λ1 * Σ|w| + λ2/2 * Σw²
```
- 두 정규화의 장점 결합
- 그룹화된 변수 선택 가능
- α 파라미터로 비율 조정

**실전 구현 (PyTorch)**
```python
# L2 정규화 (Weight Decay)
optimizer = torch.optim.Adam(model.parameters(),
                            lr=0.001,
                            weight_decay=0.01)  # λ = 0.01

# L1 정규화 (수동 추가)
l1_lambda = 0.001
l1_norm = sum(p.abs().sum() for p in model.parameters())
loss = criterion(outputs, labels) + l1_lambda * l1_norm
```

**하이퍼파라미터 선택**
- λ (정규화 강도): 0.0001 ~ 0.1 범위에서 탐색
- 교차 검증으로 최적값 결정
- 큰 λ: 과소적합 위험, 작은 λ: 과적합 위험
- 일반적으로 L2는 0.01, L1은 0.001부터 시작
