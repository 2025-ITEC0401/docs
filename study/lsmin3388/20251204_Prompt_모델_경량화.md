# 모델 경량화 학습 자료

## Prompts

### Question)
```
딥러닝 모델 경량화란 무엇이고, 왜 필요한가요? 주요 경량화 기법들에는 어떤 것들이 있나요?
```

### Answer)
**모델 경량화의 필요성:**

1. **배포 환경 제약**: 모바일, 엣지 디바이스의 제한된 리소스
2. **추론 속도**: 실시간 서비스 요구사항
3. **비용 절감**: GPU/클라우드 비용 절감
4. **에너지 효율**: 배터리 소모 감소

**주요 경량화 기법:**

**1. Pruning (가지치기)**
```python
# 중요도가 낮은 가중치를 0으로 만들거나 제거
# Structured: 전체 필터/뉴런 제거
# Unstructured: 개별 가중치 제거

import torch.nn.utils.prune as prune

# L1 Unstructured Pruning
prune.l1_unstructured(model.layer, name='weight', amount=0.3)
# 30%의 가중치 제거
```

**2. Quantization (양자화)**
```python
# 32-bit float → 8-bit int로 표현
# 메모리 4배 절약, 추론 속도 향상

# Dynamic Quantization
model_int8 = torch.quantization.quantize_dynamic(
    model, {nn.Linear, nn.LSTM}, dtype=torch.qint8
)
```

**3. Knowledge Distillation**
```python
# 큰 Teacher 모델의 지식을 작은 Student 모델로 전달
# Soft label (확률 분포) 활용
```

**4. 아키텍처 효율화**
- Depthwise Separable Convolution
- MobileNet, EfficientNet 등

**비교:**

| 기법 | 압축률 | 성능 유지 | 구현 난이도 |
|------|--------|----------|-----------|
| Pruning | 2-10x | 좋음 | 중간 |
| Quantization | 2-4x | 매우 좋음 | 낮음 |
| Distillation | 가변 | 좋음 | 중간 |

---

### Question)
```
Quantization(양자화)의 종류와 각각의 특징을 설명해주세요. INT8 양자화는 어떻게 동작하나요?
```

### Answer)
**Quantization 종류:**

**1. Post-Training Quantization (PTQ)**

학습 후 양자화를 적용합니다.

```python
# Dynamic Quantization (가장 간단)
model_quantized = torch.quantization.quantize_dynamic(
    model,
    {nn.Linear},
    dtype=torch.qint8
)

# Static Quantization (calibration 필요)
model.qconfig = torch.quantization.get_default_qconfig('fbgemm')
model_prepared = torch.quantization.prepare(model)

# Calibration: 대표 데이터로 통계 수집
for batch in calibration_loader:
    model_prepared(batch)

model_quantized = torch.quantization.convert(model_prepared)
```

**2. Quantization-Aware Training (QAT)**

학습 중에 양자화 효과를 시뮬레이션합니다.

```python
model.qconfig = torch.quantization.get_default_qat_qconfig('fbgemm')
model_prepared = torch.quantization.prepare_qat(model)

# 학습 진행 (fake quantization 적용됨)
for epoch in range(epochs):
    train(model_prepared)

model_quantized = torch.quantization.convert(model_prepared)
```

**INT8 동작 원리:**

```python
# Float32: -3.14159... (32비트)
# INT8: -128 ~ 127 (8비트)

# 양자화 수식
# q = round(x / scale) + zero_point
# x_dequant = (q - zero_point) * scale

def quantize(x, scale, zero_point):
    q = torch.round(x / scale) + zero_point
    return q.clamp(-128, 127).to(torch.int8)

def dequantize(q, scale, zero_point):
    return (q.float() - zero_point) * scale
```

**비교:**

| 방법 | 정확도 | 속도 향상 | 복잡도 |
|------|--------|----------|--------|
| Dynamic | 좋음 | 1.5-2x | 낮음 |
| Static | 매우 좋음 | 2-3x | 중간 |
| QAT | 최고 | 2-3x | 높음 |

---

### Question)
```
Knowledge Distillation은 어떻게 구현하나요? 시계열 모델에서도 적용 가능한가요?
```

### Answer)
**Knowledge Distillation 원리:**

Teacher 모델의 soft label을 Student가 학습합니다.

```python
class DistillationLoss(nn.Module):
    def __init__(self, temperature=4.0, alpha=0.5):
        super().__init__()
        self.temperature = temperature
        self.alpha = alpha
        self.kl_div = nn.KLDivLoss(reduction='batchmean')
        self.ce_loss = nn.CrossEntropyLoss()

    def forward(self, student_logits, teacher_logits, targets):
        # Soft Loss (Teacher의 soft label 학습)
        soft_targets = F.softmax(teacher_logits / self.temperature, dim=1)
        soft_student = F.log_softmax(student_logits / self.temperature, dim=1)
        soft_loss = self.kl_div(soft_student, soft_targets) * (self.temperature ** 2)

        # Hard Loss (실제 label 학습)
        hard_loss = self.ce_loss(student_logits, targets)

        # Combined Loss
        return self.alpha * soft_loss + (1 - self.alpha) * hard_loss
```

**학습 루프:**

```python
def train_distillation(teacher, student, train_loader, epochs):
    teacher.eval()  # Teacher는 학습하지 않음
    optimizer = torch.optim.Adam(student.parameters())
    criterion = DistillationLoss(temperature=4.0, alpha=0.7)

    for epoch in range(epochs):
        for inputs, targets in train_loader:
            with torch.no_grad():
                teacher_logits = teacher(inputs)

            student_logits = student(inputs)
            loss = criterion(student_logits, teacher_logits, targets)

            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
```

**시계열에서의 적용:**

```python
class TimeSeriesDistillation:
    """시계열 예측을 위한 Distillation"""

    def __init__(self, teacher, student, temperature=2.0):
        self.teacher = teacher
        self.student = student
        self.temperature = temperature

    def distill_loss(self, x, y):
        with torch.no_grad():
            teacher_pred = self.teacher(x)

        student_pred = self.student(x)

        # MSE with Teacher
        distill_loss = F.mse_loss(student_pred, teacher_pred)

        # MSE with Ground Truth
        task_loss = F.mse_loss(student_pred, y)

        return 0.5 * distill_loss + 0.5 * task_loss
```

**시계열 Distillation 변형:**

1. **Feature Distillation**: 중간 층의 표현도 전달
2. **Attention Distillation**: Attention 패턴 전달
3. **Temporal Distillation**: 시간 축 특성 보존

```python
# Attention Distillation 예시
def attention_distillation_loss(student_attn, teacher_attn):
    # Attention 분포의 KL divergence
    return F.kl_div(
        F.log_softmax(student_attn, dim=-1),
        F.softmax(teacher_attn, dim=-1),
        reduction='batchmean'
    )
```
