# Learning Rate Warmup 학습 자료

## Prompts

### Question)
```
Learning Rate Warmup이란 무엇이고, 왜 필요한가요? 특히 Transformer 모델에서 중요한 이유는 무엇인가요?
```

### Answer)
**Learning Rate Warmup 정의**

학습 초기에 매우 작은 학습률에서 시작하여 점진적으로 목표 학습률까지 증가시키는 기법입니다.

**필요한 이유:**
1. **초기 불안정성 방지**: 학습 초기 파라미터가 무작위로 초기화되어 있어 큰 학습률은 발산 위험
2. **Adam의 분산 추정 문제**: 초기에 2차 모멘텀 추정이 부정확하여 학습률이 과대평가됨
3. **안정적인 수렴**: 점진적 증가로 최적화 경로 안정화

**Transformer에서 특히 중요한 이유:**
- 깊은 네트워크 구조로 gradient 크기 변동이 큼
- Self-attention의 softmax가 초기에 불안정
- Layer Normalization과 함께 사용 시 시너지
- 큰 학습률(1e-3 ~ 1e-4)을 사용하므로 초기 충격 완화 필요

```python
# 선형 Warmup 구현
def get_linear_warmup_scheduler(optimizer, warmup_steps, total_steps):
    def lr_lambda(step):
        if step < warmup_steps:
            return step / warmup_steps
        return 1.0

    return torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)

# 사용 예시
warmup_steps = 1000
total_steps = 100000
scheduler = get_linear_warmup_scheduler(optimizer, warmup_steps, total_steps)
```

---

### Question)
```
Warmup 방식의 종류(Linear, Exponential, Cosine)를 비교해주세요. 어떤 상황에서 어떤 방식을 선택하나요?
```

### Answer)
**1. Linear Warmup**
```
lr = base_lr * (step / warmup_steps)
```
- 가장 기본적이고 널리 사용됨
- 일정한 속도로 학습률 증가
- BERT, GPT 등 대부분의 Transformer에서 사용

**2. Exponential Warmup**
```
lr = base_lr * (1 - exp(-step / warmup_steps))
```
- 초기에 빠르게 증가, 후반에 완만
- 더 부드러운 전이

**3. Cosine Warmup**
```
lr = base_lr * 0.5 * (1 - cos(π * step / warmup_steps))
```
- S자 곡선 형태
- Cosine Annealing과 자연스럽게 연결

**비교 및 선택 가이드:**

| 방식 | 특징 | 권장 상황 |
|------|------|----------|
| Linear | 단순, 예측 가능 | 기본 선택, BERT/GPT |
| Exponential | 빠른 초기 증가 | 빠른 수렴 필요 시 |
| Cosine | 부드러운 전이 | Cosine decay와 함께 |

```python
# PyTorch에서 Linear Warmup + Cosine Decay
from transformers import get_cosine_schedule_with_warmup

scheduler = get_cosine_schedule_with_warmup(
    optimizer,
    num_warmup_steps=warmup_steps,
    num_training_steps=total_steps
)
```

---

### Question)
```
Warmup 기간(warmup steps)은 어떻게 설정하나요? 일반적인 권장 값이 있나요?
```

### Answer)
**Warmup 기간 설정 가이드라인:**

**비율 기반 설정:**
- 전체 학습 스텝의 1%~10%
- 일반적으로 5%~6% 권장

**절대값 기반 설정:**
- 소규모 모델: 500~1,000 스텝
- 중규모 모델: 2,000~4,000 스텝
- 대규모 모델(GPT-3 등): 10,000+ 스텝

**모델/태스크별 권장:**

| 모델/태스크 | Warmup Steps | 비율 |
|------------|--------------|------|
| BERT Fine-tuning | 전체의 6% | 1 epoch 이내 |
| GPT-2 Pre-training | 2,000 | ~0.1% |
| Vision Transformer | 10,000 | ~3% |
| 시계열 모델 | 500~1,000 | 5% |

**설정 시 고려사항:**
1. 배치 크기가 크면 warmup 더 길게
2. 학습률이 높으면 warmup 더 길게
3. 데이터셋이 크면 비율 낮춰도 됨
4. Fine-tuning은 Pre-training보다 짧게

```python
# 예시: 전체 스텝의 6%를 warmup으로
total_steps = len(train_loader) * num_epochs
warmup_steps = int(0.06 * total_steps)

# 또는 첫 에포크를 warmup으로
warmup_steps = len(train_loader)  # 1 epoch
```

**실험적 조정:**
- 학습 초기 loss가 발산하면 warmup 늘리기
- 수렴이 너무 느리면 warmup 줄이기
- Learning rate finder와 함께 사용하여 최적값 탐색
